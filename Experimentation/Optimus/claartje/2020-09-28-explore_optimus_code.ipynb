{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 PRELIMINARIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "from argparse import Namespace\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "\n",
    "from relevant_optimus_code.configuration_bert import BertConfig\n",
    "from relevant_optimus_code.configuration_gpt2 import GPT2Config\n",
    "from relevant_optimus_code.tokenization_bert import BertTokenizer\n",
    "from relevant_optimus_code.tokenization_gpt2 import GPT2Tokenizer\n",
    "from relevant_optimus_code.modeling_bert import BertForLatentConnector\n",
    "from relevant_optimus_code.modeling_gpt2 import GPT2ForLatentConnector\n",
    "from relevant_optimus_code.run_latent_generation import add_special_tokens_to_decoder, interpolate, latent_code_from_text, text_from_latent_code, top_k_top_p_filtering\n",
    "from relevant_optimus_code.vae import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "# GLOBALS\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2ForLatentConnector, GPT2Tokenizer),\n",
    "    'bert': (BertConfig, BertForLatentConnector, BertTokenizer)}\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"); print(\"DEVICE:\", DEVICE)\n",
    "SEED = 42; np.random.seed(SEED); torch.manual_seed(SEED);\n",
    "ENCODER_MODEL_TYPE = 'bert'\n",
    "ENCODER_MODEL_NAME = 'bert-base-cased'\n",
    "DECODER_MODEL_TYPE = 'gpt2'\n",
    "DECODER_MODEL_NAME = 'gpt2'\n",
    "MAX_LEN_EX_SPECIAL = 510\n",
    "\n",
    "GLOBAL_STEP = 31250\n",
    "LATENT_SIZE = 768\n",
    "DO_LOWERCASE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# PATHS\n",
    "PREFIX_PATH = '/Users/claartje/Dropbox (Persoonlijk)/Studie/Master AI/Thesis/Thesis_MSc_AI/Experimentation/Optimus/'\n",
    "DATA_DIR = 'claartje/data/'\n",
    "DATA_FILES = {'train':'train.txt', 'valid':'valid.txt', 'test':'test.txt'}\n",
    "CHECKPOINT_DIRS = {'B{}'.format(v):'{}output/LM/Snli/768/philly_vae_snli_b{}_d5_r00.5_ra0.25_length_weighted/checkpoint-31250/'.format(PREFIX_PATH, v) for v in [0.0, 0.5, 1.0]}\n",
    "OUTPUT_ENCODER_DIR = {'B{}'.format(v):CHECKPOINT_DIRS['B{}'.format(v)]+'checkpoint-encoder-{}'.format(GLOBAL_STEP) for v in [0.0, 0.5, 1.0]}\n",
    "OUTPUT_DECODER_DIR = {'B{}'.format(v):CHECKPOINT_DIRS['B{}'.format(v)]+'checkpoint-decoder-{}'.format(GLOBAL_STEP) for v in [0.0, 0.5, 1.0]}\n",
    "OUTPUT_FULL_DIR = {'B{}'.format(v):CHECKPOINT_DIRS['B{}'.format(v)]+'checkpoint-full-{}'.format(GLOBAL_STEP) for v in [0.0, 0.5, 1.0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 INITIALISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Model (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to initialise models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_model_tokenizer_encoder(beta_name=\"B1.0\"):\n",
    "    # Load a trained Encoder model and vocabulary that you have fine-tuned\n",
    "    encoder_config_class, encoder_model_class, encoder_tokenizer_class = MODEL_CLASSES[ENCODER_MODEL_TYPE]\n",
    "    # print(\"Loading:\", OUTPUT_ENCODER_DIR[beta_name])\n",
    "    model_encoder = encoder_model_class.from_pretrained(OUTPUT_ENCODER_DIR[beta_name], latent_size=LATENT_SIZE)\n",
    "    tokenizer_encoder = encoder_tokenizer_class.from_pretrained(ENCODER_MODEL_NAME, do_lower_case=DO_LOWERCASE)\n",
    "    return model_encoder, tokenizer_encoder\n",
    "\n",
    "def get_model_tokenizer_decoder(beta_name=\"B1.0\"):\n",
    "    # Load a trained Decoder model and vocabulary that you have fine-tuned\n",
    "    decoder_config_class, decoder_model_class, decoder_tokenizer_class = MODEL_CLASSES[DECODER_MODEL_TYPE]\n",
    "    # print(\"Loading: \", OUTPUT_DECODER_DIR[beta_name])\n",
    "    model_decoder = decoder_model_class.from_pretrained(OUTPUT_DECODER_DIR[beta_name], latent_size=LATENT_SIZE)\n",
    "    tokenizer_decoder = decoder_tokenizer_class.from_pretrained(DECODER_MODEL_NAME, do_lower_case=DO_LOWERCASE)\n",
    "    model_decoder, tokenizer_decoder = add_special_tokens_to_decoder(model_decoder, tokenizer_decoder)\n",
    "    return model_decoder, tokenizer_decoder\n",
    "\n",
    "def get_model_vae(model_encoder, model_decoder, tokenizer_encoder, tokenizer_decoder, beta_name=\"B1.0\"):\n",
    "    checkpoint_full = torch.load(os.path.join(OUTPUT_FULL_DIR[beta_name], 'training.bin'), map_location=torch.device(DEVICE))\n",
    "    # print(\"Loading: \", OUTPUT_FULL_DIR[beta_name])\n",
    "    args = {'latent_size':LATENT_SIZE, 'device':DEVICE}\n",
    "    model_vae = VAE(model_encoder, model_decoder, tokenizer_encoder, tokenizer_decoder, Namespace(**args))\n",
    "    model_vae.load_state_dict(checkpoint_full['model_state_dict'])\n",
    "    return model_vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init models for different beta values: 0.0, 0.5 and 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG: {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "CONFIG: {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"latent_size\": 768,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"vocab_size\": 50260\n",
      "}\n",
      "\n",
      "CONFIG: {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "CONFIG: {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"latent_size\": 768,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"vocab_size\": 50260\n",
      "}\n",
      "\n",
      "CONFIG: {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "CONFIG: {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"latent_size\": 768,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"vocab_size\": 50260\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_models, encoder_models, vae_models = {}, {}, {}\n",
    "\n",
    "for beta in [0.0, 0.5, 1.0]:\n",
    "    beta_name = \"B{}\".format(beta)\n",
    "    \n",
    "    model_encoder, tokenizer_encoder = get_model_tokenizer_encoder(beta_name=beta_name)\n",
    "    model_decoder, tokenizer_decoder = get_model_tokenizer_decoder(beta_name=beta_name)\n",
    "    \n",
    "    decoder_models[beta_name] = model_decoder\n",
    "    encoder_models[beta_name] = model_encoder\n",
    "    \n",
    "    vae_models[beta_name] = get_model_vae(model_encoder, model_decoder, tokenizer_encoder, tokenizer_decoder, beta_name=beta_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of parameters VAE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/claartje/.cache/torch/hub/huggingface_pytorch-transformers_master\n",
      "Using cache found in /Users/claartje/.cache/torch/hub/huggingface_pytorch-transformers_master\n",
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the full VAE:\t\t\t 241599744\n",
      "Number of parameters in standard BERT + GPT2:\t\t 233922048\n",
      "\n",
      "Number of parameters in the decoder (GPT2):\t\t 132109824\n",
      "Number of parameters in a standard decoder (GPT2):\t 124439808\n",
      "\n",
      "Number of parameters in the encoder (BERT):\t\t 109489920\n",
      "Number of parameters in a standard encoder (BERT):\t 109482240\n"
     ]
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "standard_bert = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')\n",
    "standard_gpt2 = torch.hub.load('huggingface/pytorch-transformers', 'model', 'gpt2')\n",
    "\n",
    "print(\"Number of parameters in the full VAE:\\t\\t\\t\", get_n_params(vae_models[\"B1.0\"]))\n",
    "print(\"Number of parameters in standard BERT + GPT2:\\t\\t\", get_n_params(standard_gpt2) +  get_n_params(standard_bert))\n",
    "\n",
    "print(\"\\nNumber of parameters in the decoder (GPT2):\\t\\t\", get_n_params(decoder_models[\"B1.0\"]))\n",
    "print(\"Number of parameters in a standard decoder (GPT2):\\t\", get_n_params(standard_gpt2))\n",
    "print(\"\\nNumber of parameters in the encoder (BERT):\\t\\t\", get_n_params(encoder_models[\"B1.0\"]))\n",
    "print(\"Number of parameters in a standard encoder (BERT):\\t\", get_n_params(standard_bert))\n",
    "\n",
    "del standard_bert\n",
    "del standard_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate with different beta value models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am very happy --> He is terribly sad\n",
      "******************************\n",
      "Interpolation for BETA: 0.0\n",
      "-\n",
      "0 I am very happy\n",
      "1 there is very happy\n",
      "2 it is very happy\n",
      "3 there is very happy person\n",
      "4 it is very happy\n",
      "5 there is very happy man\n",
      "6 he is very sad\n",
      "7 he is very unhappy\n",
      "8 he is very unhappy\n",
      "9 he is terribly sad\n",
      "10 he is terribly sad\n",
      "11 he is terribly sad\n",
      "\n",
      "******************************\n",
      "Interpolation for BETA: 0.5\n",
      "-\n",
      "0 I am very happy\n",
      "1 there is very happy person\n",
      "2 there is very happy woman\n",
      "3 it is very happy\n",
      "4 it is very happy\n",
      "5 it is very sad\n",
      "6 he is very happy\n",
      "7 he is very unhappy\n",
      "8 he is very sad\n",
      "9 he is very sad\n",
      "10 he is terribly sad\n",
      "11 he is terribly sad\n",
      "\n",
      "******************************\n",
      "Interpolation for BETA: 1.0\n",
      "-\n",
      "0 I am very happy\n",
      "1 there is very happy person\n",
      "2 there is very happy woman\n",
      "3 this is very happy\n",
      "4 it is very happy\n",
      "5 it is very happy\n",
      "6 it is quite sad\n",
      "7 he is very sad\n",
      "8 he is very unhappy\n",
      "9 he is terribly sad\n",
      "10 he is very sad\n",
      "11 he is terribly sad\n",
      "\n",
      "The man walks up the stairs to the woman --> Three women greet a man that walks on the street\n",
      "******************************\n",
      "Interpolation for BETA: 0.0\n",
      "-\n",
      "0 The man walks up the stairs to the woman\n",
      "1 the man walks up the stairs to the woman\n",
      "2 the man walks up the stairs to the woman\n",
      "3 the man walks up the stairs to the woman\n",
      "4 the man walks up the stairs to the woman\n",
      "5 a man walks up the stairs to the woman\n",
      "6 a man walks up the stairs for a woman\n",
      "7 a woman walk up the stairs for a man\n",
      "8 three women walk up the stairs for a man\n",
      "9 three women greet a man that walks on the street\n",
      "10 three women greet a man who walks on the street\n",
      "11 three women greet a man who walks on the street\n",
      "\n",
      "******************************\n",
      "Interpolation for BETA: 0.5\n",
      "-\n",
      "0 The man walks up the stairs to the woman\n",
      "1 the man walks up the stairs to the woman\n",
      "2 the man walks up the stairs to the woman\n",
      "3 the man walks up the stairs to the woman\n",
      "4 the man walks up the stairs to the woman\n",
      "5 a men walk up the stairs to the woman\n",
      "6 a woman walk up the stairs for a man\n",
      "7 three women walk up the stairs for a man\n",
      "8 three women greet a man walking on the stairs\n",
      "9 three women walk a man that greets on the stairs\n",
      "10 three women greet a man who walks on the street\n",
      "11 three women greet a man who walks on the street\n",
      "\n",
      "******************************\n",
      "Interpolation for BETA: 1.0\n",
      "-\n",
      "0 The man walks up the stairs to the woman\n",
      "1 the man walks up the stairs to the woman\n",
      "2 the man walks up the stairs to the woman\n",
      "3 the men walks up the stairs to the woman\n",
      "4 the man walks up the stairs to the woman\n",
      "5 the man walks up the stairs to greet a woman\n",
      "6 a woman walks up the stairs for a man\n",
      "7 three women walk up the stairs for a man\n",
      "8 three women walk up the street for a man\n",
      "9 three women walk a man about the greeting\n",
      "10 three women greet a man walking on the street\n",
      "11 three women greet a man who walks on the street\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def interpolate_different_beta(source_sentence, target_sentence, \n",
    "                               vae_models, tokenizer_encoder, tokenizer_decoder,\n",
    "                               num_interpolation_steps=9, temperature=1.0, top_k=0, top_p=1.0):\n",
    "    for beta in [0.0, 0.5, 1.0]:\n",
    "        beta_name = \"B{}\".format(beta)\n",
    "        print('*' * 30)\n",
    "        print(\"Interpolation for BETA: {}\".format(beta))\n",
    "        print('-')\n",
    "        print(\"0\", source_sentence)\n",
    "        result = interpolate(vae_models[beta_name], tokenizer_encoder, tokenizer_decoder, source_sentence, target_sentence, DEVICE, num_interpolation_steps, top_k, top_p, temperature)\n",
    "        print()\n",
    "\n",
    "sentence_pairs = [[\"I am very happy\", \"He is terribly sad\"], [\"The man walks up the stairs to the woman\", \"Three women greet a man that walks on the street\"]]\n",
    "\n",
    "for p in sentence_pairs:\n",
    "    print(p[0], '-->', p[1])\n",
    "    interpolate_different_beta(p[0], p[1], vae_models, tokenizer_encoder, tokenizer_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto encode some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_text(input_text, tokenizer_encoder, tokenizer_decoder, vae_models, beta_name, verbose=False):\n",
    "    s = time.time()\n",
    "\n",
    "    words_random_text = input_text.split(' ')\n",
    "\n",
    "    latent_z, _ = latent_code_from_text(\" \".join(words_random_text[:318]), tokenizer_encoder, vae_models[beta_name], DEVICE)\n",
    "    reconstructed_text = text_from_latent_code(latent_z, vae_models[beta_name], DEVICE, 0, 1.0, 1.0, tokenizer_decoder)\n",
    "    if verbose:\n",
    "        print(\"Input text:\\n\\n\", input_text)\n",
    "        print(\"\\n\\nLength of the text:\", len(words_random_text))\n",
    "        print(\"\\n\\nReconstruction of the text:\\n\\n\", reconstructed_text)\n",
    "        print(\"\\nTook {:.2f} seconds to reconstruct the input.\".format(time.time() - s))\n",
    "    return latent_z, reconstructed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## Longer text (325 words, 510 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "Input text:\n",
      "\n",
      " The epic, traditionally ascribed to the Maharishi Valmiki, narrates the life of Rama, prince of the legendary kingdom of Kosala. The story follows his fourteen-year exile to the forest urged by his father King Dasharatha, on the request of Rama's stepmother Kaikeyi; his travels across forests in India with his wife Sita and brother Lakshmana, the kidnapping of Sita by Ravana --the evil king of Lanka, that resulted in war (against evil); and Rama's eventual return to Ayodhya to be crowned king amidst jubilation and celebration. This is the culmination point of the epic. It is considered a sacred book, and is read by millions of people every year.\n",
      "There have been many attempts to unravel the epic's historical growth and compositional layers; various recent scholars' estimates for the earliest stage of the text range from the 7th to 4th centuries BCE,[5] with later stages extending up to the 3rd century CE.[6]\n",
      "The Ramayana is one of the largest ancient epics in world literature. It consists of nearly 24,000 verses (mostly set in the Shloka/Anustubh meter), divided into five kāṇḍas: the ayodhyakāṇḍa, the araṇyakāṇḍa, the kiṣkindakāṇḍa, the sundarākāṇḍa, and the laṅkākāṇḍa. and about 500 sargas (chapters).The uttarākāṇḍa,the bālakāṇḍa, although frequently counted among the main ones, is not a part of the original epic. Though Balakanda sometimes considered in the main epic, but according to many, Uttara Kanda is a later interpolation and thus it's not attributed to the work of Maharshi Valmiki.[7] In Hindu tradition, the Ramayana is considered to be the Adi-kavya (first poem). It depicts the duties of relationships, portraying ideal characters like the ideal father, the ideal servant, the ideal brother, the ideal husband and the ideal king. The Ramayana was an important influence on later Sanskrit poetry and Hindu life and culture. Its most important moral influence was the importance of virtue, in the life of a citizen and in the ideals of the formation of a state or of a functioning society.\n",
      "\n",
      "\n",
      "Length of the text: 325\n",
      "\n",
      "\n",
      "Reconstruction of the text:\n",
      "\n",
      " the brother mammon, the hobo, and the famous beagle today, are made darker during the clerics descent of roosts function where california, the menting author decides to pose long into the lake, where also, because of many cycles conclusively, the ideo and tinderella, the central theme is castle, conundra, india, traversing dragons ) of valley,\n",
      "\n",
      "Took 16.44 seconds to reconstruct the input.\n"
     ]
    }
   ],
   "source": [
    "random_text = '''The epic, traditionally ascribed to the Maharishi Valmiki, narrates the life of Rama, prince of the legendary kingdom of Kosala. The story follows his fourteen-year exile to the forest urged by his father King Dasharatha, on the request of Rama's stepmother Kaikeyi; his travels across forests in India with his wife Sita and brother Lakshmana, the kidnapping of Sita by Ravana --the evil king of Lanka, that resulted in war (against evil); and Rama's eventual return to Ayodhya to be crowned king amidst jubilation and celebration. This is the culmination point of the epic. It is considered a sacred book, and is read by millions of people every year.\n",
    "There have been many attempts to unravel the epic's historical growth and compositional layers; various recent scholars' estimates for the earliest stage of the text range from the 7th to 4th centuries BCE,[5] with later stages extending up to the 3rd century CE.[6]\n",
    "The Ramayana is one of the largest ancient epics in world literature. It consists of nearly 24,000 verses (mostly set in the Shloka/Anustubh meter), divided into five kāṇḍas: the ayodhyakāṇḍa, the araṇyakāṇḍa, the kiṣkindakāṇḍa, the sundarākāṇḍa, and the laṅkākāṇḍa. and about 500 sargas (chapters).The uttarākāṇḍa,the bālakāṇḍa, although frequently counted among the main ones, is not a part of the original epic. Though Balakanda sometimes considered in the main epic, but according to many, Uttara Kanda is a later interpolation and thus it's not attributed to the work of Maharshi Valmiki.[7] In Hindu tradition, the Ramayana is considered to be the Adi-kavya (first poem). It depicts the duties of relationships, portraying ideal characters like the ideal father, the ideal servant, the ideal brother, the ideal husband and the ideal king. The Ramayana was an important influence on later Sanskrit poetry and Hindu life and culture. Its most important moral influence was the importance of virtue, in the life of a citizen and in the ideals of the formation of a state or of a functioning society.'''\n",
    "\n",
    "_, _ = reconstruct_text(random_text, tokenizer_encoder, tokenizer_decoder, vae_models, \"B1.0\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "\n",
      " We investigate whether the considered unsupervised disentanglement approaches are effective at enforcing a factorizing and thus uncorrelated aggregated posterior. For each trained model, we sample 10 000 images and compute a sample from the corresponding approximate posterior. We then fit a multivariate Gaussian distribution over these 10 000 samples by computing the empirical mean and covariance matrix. Finally, we compute the total correlation of the fitted Gaussian and report the median value for each data set, method and hyperparameter value.\n",
      "\n",
      "\n",
      "Length of the text: 80\n",
      "\n",
      "\n",
      "Reconstruction of the text:\n",
      "\n",
      " this test examines the different techniques and methodologies vying at results from a eton future branded plugging objects, fully incline what most london, the mic, being held to enter the rounded pound for <unk>.\n",
      "\n",
      "Took 5.47 seconds to reconstruct the input.\n"
     ]
    }
   ],
   "source": [
    "random_text_2 = '''We investigate whether the considered unsupervised disentanglement approaches are effective at enforcing a factorizing and thus uncorrelated aggregated posterior. For each trained model, we sample 10 000 images and compute a sample from the corresponding approximate posterior. We then fit a multivariate Gaussian distribution over these 10 000 samples by computing the empirical mean and covariance matrix. Finally, we compute the total correlation of the fitted Gaussian and report the median value for each data set, method and hyperparameter value.'''\n",
    "\n",
    "_, _ = reconstruct_text(random_text_2, tokenizer_encoder, tokenizer_decoder, vae_models, \"B1.0\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "\n",
      " We investigate whether the considered unsupervised disentanglement approaches are effective at enforcing a factorizing and thus uncorrelated aggregated posterior .\n",
      "\n",
      "\n",
      "Length of the text: 20\n",
      "\n",
      "\n",
      "Reconstruction of the text:\n",
      "\n",
      " the evaluation of these substances is conforming to differing <unk> <unk> instead of a attentive restrained cautionary call.\n",
      "\n",
      "Took 2.39 seconds to reconstruct the input.\n"
     ]
    }
   ],
   "source": [
    "random_text_3 = '''We investigate whether the considered unsupervised disentanglement approaches are effective at enforcing a factorizing and thus uncorrelated aggregated posterior .'''\n",
    "\n",
    "_, _ = reconstruct_text(random_text_3, tokenizer_encoder, tokenizer_decoder, vae_models, \"B1.0\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "\n",
      " This is an apple .\n",
      "\n",
      "\n",
      "Length of the text: 5\n",
      "\n",
      "\n",
      "Reconstruction of the text:\n",
      "\n",
      " this is an apple.\n",
      "\n",
      "Took 0.55 seconds to reconstruct the input.\n",
      "--------------------------------------------------\n",
      "Input text:\n",
      "\n",
      " This is an apple .\n",
      "\n",
      "\n",
      "Length of the text: 5\n",
      "\n",
      "\n",
      "Reconstruction of the text:\n",
      "\n",
      " this is an apple.\n",
      "\n",
      "Took 0.55 seconds to reconstruct the input.\n",
      "--------------------------------------------------\n",
      "Input text:\n",
      "\n",
      " This is an apple .\n",
      "\n",
      "\n",
      "Length of the text: 5\n",
      "\n",
      "\n",
      "Reconstruction of the text:\n",
      "\n",
      " this is an apple.\n",
      "\n",
      "Took 0.48 seconds to reconstruct the input.\n"
     ]
    }
   ],
   "source": [
    "random_text_4 = '''This is an apple .'''\n",
    "\n",
    "_, _ = reconstruct_text(random_text_4, tokenizer_encoder, tokenizer_decoder, vae_models, \"B0.0\", verbose=True);print('-'*50)\n",
    "_, _ = reconstruct_text(random_text_4, tokenizer_encoder, tokenizer_decoder, vae_models, \"B0.5\", verbose=True);print('-'*50)\n",
    "_, _ = reconstruct_text(random_text_4, tokenizer_encoder, tokenizer_decoder, vae_models, \"B1.0\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode a set of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sentence_from_tok_ids(tok_ids_tensor):\n",
    "    text = tokenizer_decoder.decode(tok_ids_tensor.tolist(), clean_up_tokenization_spaces=True)\n",
    "    text = text.split()[1:-1]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def text_from_latent_code_batch(latents, tokenizer_decoder, model_vae, \n",
    "                                top_p=1.0, top_k=0, temperature=1.0, \n",
    "                                batch_size=300, max_sentence_length=4):\n",
    "    N_batches = int(np.ceil(len(sentences) / batch_size))\n",
    "    \n",
    "    decoded = []\n",
    " \n",
    "    for batch_i, batch_latents in enumerate(torch.chunk(latents, N_batches, dim=0)):\n",
    "        print(\"DECODE - Batch {:03d}\".format(batch_i))\n",
    "        generated_so_far = torch.tensor(tokenizer_decoder.added_tokens_encoder['<BOS>'], dtype=torch.long, device=DEVICE).unsqueeze(0).repeat(batch_latents.shape[0], 1)\n",
    "        for i in range(max_sentence_length):\n",
    "            print('word', i, end='\\r')\n",
    "            outputs = model_vae.decoder(generated_so_far, past=batch_latents)\n",
    "            next_token_logits = outputs[0][:, 0, :]\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=1)\n",
    "            next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            generated_so_far = torch.cat((generated_so_far, next_token), dim=1)\n",
    "        print(\"end generation batch shape:\", generated_so_far.shape)\n",
    "        decoded.append(generated_so_far)\n",
    "    return torch.cat(decoded)\n",
    "\n",
    "def tokenize_pad_batch(sentences):\n",
    "    tokenized_sentences = []\n",
    "    max_len = 0\n",
    "    \n",
    "    for i, s in enumerate(sentences):\n",
    "        tokenized1 =  [101] + tokenizer_encoder.encode(s) + [102]\n",
    "        if len(tokenized1) > max_len:\n",
    "            max_len = len(tokenized1)\n",
    "        tokenized_sentences.append(tokenized1)\n",
    "\n",
    "    padded_tokenized_sentences = []\n",
    "    for ts in tokenized_sentences:\n",
    "        padding = [tokenizer_encoder.vocab['[PAD]']] * (max_len - len(ts))\n",
    "        padded_tokenized_sentences.append(ts + padding)\n",
    "\n",
    "    return torch.tensor(padded_tokenized_sentences), tokenized_sentences, max_len\n",
    "\n",
    "def latent_code_from_text_batch(pad_tok_sentences, tokenizer_encoder, model_vae, batch_size=500):\n",
    "    N_batches = np.ceil(pad_tok_sentences.shape[0] / batch_size)\n",
    "    print(\"{} sentences in total, with max batch size of {} gives {} batches\".format(len(sentences), batch_size, N_batches))\n",
    "    \n",
    "    latents = []\n",
    "    for batch_i, batch in enumerate(torch.chunk(pad_tok_sentences, int(N_batches), dim=0)):\n",
    "        print(\"ENCODE - Batch {:03d}\".format(batch_i), end='\\r')\n",
    "        \n",
    "        coded1 = torch.Tensor.long(batch)\n",
    "        with torch.no_grad():\n",
    "            x0 = coded1\n",
    "            x0 = x0.to(DEVICE)\n",
    "            _, pooled_hidden_fea = model_vae.encoder(x0, attention_mask=(x0 > 0).float())  #\n",
    "            mean, logvar = model_vae.encoder.linear(pooled_hidden_fea).chunk(2, -1)\n",
    "            latent_z = mean.squeeze(1)\n",
    "            latents.append(latent_z)\n",
    "    \n",
    "    return torch.cat(latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_N_sentences = True\n",
    "N_sentences = 200\n",
    "sentences_txt_file = 'sample_text.txt'\n",
    "\n",
    "sentences = []\n",
    "with open(sentences_txt_file, 'r') as fd:\n",
    "    reader = csv.reader(fd, delimiter='\\t')\n",
    "    for i, row in enumerate(reader):\n",
    "        sentences.append(row[1])\n",
    "        sentences.append(row[2])\n",
    "        if max_N_sentences:\n",
    "            if (len(sentences)>= N_sentences):\n",
    "                break\n",
    "\n",
    "padded_tokenised_sentences, tokenised_sentences, max_len_sentences = tokenize_pad_batch(sentences)\n",
    "print(\"Maximum sentence length:\", max_len_sentences)\n",
    "print(\"Padded tokenised sentences block shape:\", padded_tokenised_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "B0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "402 sentences in total, with max batch size of 500 gives 1.0 batches\n",
      "DECODE - Batch 000\n",
      "end generation batch shape: torch.Size([402, 11])\n",
      "****************************************************************************************************\n",
      "B0.5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "402 sentences in total, with max batch size of 500 gives 1.0 batches\n",
      "DECODE - Batch 000\n",
      "end generation batch shape: torch.Size([402, 11])\n",
      "****************************************************************************************************\n",
      "B1.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "402 sentences in total, with max batch size of 500 gives 1.0 batches\n",
      "DECODE - Batch 000\n",
      "end generation batch shape: torch.Size([402, 11])\n"
     ]
    }
   ],
   "source": [
    "process_sentences = True            \n",
    "\n",
    "if process_sentences:\n",
    "        \n",
    "    latents = {\"B1.0\":[], \"B0.5\":[], \"B0.0\":[]}\n",
    "    decoded_padded = {\"B1.0\":[], \"B0.5\":[], \"B0.0\":[]}\n",
    "\n",
    "    for beta in [0.0, 0.5, 1.0]:\n",
    "        print(\"*\"*100)\n",
    "        beta_name = \"B{}\".format(beta)\n",
    "        \n",
    "        print(beta_name)\n",
    "        print(\"-\"*100)\n",
    "\n",
    "        z = latent_code_from_text_batch(padded_tokenised_sentences, tokenizer_encoder, vae_models[beta_name], batch_size=500)\n",
    "        latents[beta_name] = z\n",
    "                \n",
    "        dec = text_from_latent_code_batch(z, tokenizer_decoder, vae_models[beta_name], batch_size=500, \n",
    "                                                     max_sentence_length=10)\n",
    "        decoded_padded[beta_name] = dec\n",
    "        \n",
    "#     with open('latents.pickle', 'wb') as handle:\n",
    "#         pickle.dump(latents, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "# else:\n",
    "#     with open('latents.pickle', 'rb') as handle:\n",
    "#         latents = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "------ 0 ------\n",
      "Original sentence: Tasting it is the only reliable way.\n",
      "Reconstructed sentence: it is locking the so called best weather.\n",
      "torch.Size([1, 768])\n",
      "Reconstructed sentence 2: it is adjusting the only wiener safe.\n",
      "------ 1 ------\n",
      "Original sentence: The way you have it is fine.\n",
      "Reconstructed sentence: the way it is made of sand.\n",
      "torch.Size([1, 768])\n",
      "Reconstructed sentence 2: the way that you have is fine.\n",
      "------ 2 ------\n",
      "Original sentence: I think it probably depends on your money.\n",
      "Reconstructed sentence: it thinks that it should cost nothing.\n",
      "torch.Size([1, 768])\n",
      "Reconstructed sentence 2: it thinks it might cost me one penny.\n",
      "------ 3 ------\n",
      "Original sentence: It depends on your country.\n",
      "Reconstructed sentence: it chooses itself on the country.\n",
      "torch.Size([1, 768])\n",
      "Reconstructed sentence 2: it chooses on itself.\n",
      "------ 4 ------\n",
      "Original sentence: You need to read a lot to know what you like and what you don't.\n",
      "Reconstructed sentence: i want to know how many people like to read a book and have a cup of juice.\n",
      "torch.Size([1, 768])\n",
      "Reconstructed sentence 2: i pick a thing to read and enjoy but don't have the chance to read it.\n",
      "------ 5 ------\n",
      "Original sentence: You don't have to know.\n",
      "Reconstructed sentence: nobody can be <unk>.\n",
      "torch.Size([1, 768])\n",
      "Reconstructed sentence 2: nobody can do <unk>.\n",
      "------ 6 ------\n",
      "Original sentence: Obviously, the best book for you depends a lot on what you are looking for.\n",
      "Reconstructed sentence: this sort of book is always nice for a certain person, but the only way to check on his scores.\n",
      "torch.Size([1, 768])\n",
      "Reconstructed sentence 2: this nicely crafted prize for the most important thing is to read, no one else.\n",
      "------ 7 ------\n",
      "Original sentence: The answer will depend of course on what you're looking for in a vacation.\n",
      "Reconstructed sentence: the answer to what you need are pictured below, for a leisurely day.\n",
      "torch.Size([1, 768])\n",
      "Reconstructed sentence 2: the answer to questions are determined by what you should're enjoying in a flight of leisure.\n",
      "------ 8 ------\n",
      "Original sentence: I've had this same problem.\n",
      "Reconstructed sentence: that's had both my age.\n",
      "torch.Size([1, 768])\n",
      "Reconstructed sentence 2: this's had some kind of illness.\n",
      "------ 9 ------\n",
      "Original sentence: I had the same problem as you.\n",
      "Reconstructed sentence: it was the correct one with energy.\n",
      "torch.Size([1, 768])\n",
      "Reconstructed sentence 2: it had the same asian problem.\n",
      "------ 10 ------\n",
      "Original sentence: If you are not sure how to do it, don't do it at all.\n",
      "Reconstructed sentence: if you don't know how to do anything, then you will not be able to do anything at all.\n",
      "torch.Size([1, 768])\n",
      "Reconstructed sentence 2: no one is just don't know how to do anything at all, it's possible.\n"
     ]
    }
   ],
   "source": [
    "reconstructed_text = {\"B1.0\":[], \"B0.5\":[], \"B0.0\":[]}\n",
    "\n",
    "for beta in [0.0, 0.5, 1.0]:\n",
    "    print(\"*\"*100)\n",
    "    beta_name = \"B{}\".format(beta)\n",
    "    for i, pad_dec_sequence in enumerate(decoded_padded[beta_name]):\n",
    "        print(\"------ {} ------\".format(i))\n",
    "        text = sentence_from_tok_ids(pad_dec_sequence)\n",
    "        print(\"Original sentence:\", sentences[i])\n",
    "#         print(\"Batch reconstructed sentence:\", text)\n",
    "        z = latents[beta_name][i, :].unsqueeze(0)\n",
    "        t = text_from_latent_code(z, vae_models[beta_name], DEVICE, 0, 1.0, 1.0, tokenizer_decoder)\n",
    "        print(\"Reconstructed sentence:\", t)\n",
    "        _, t2 = reconstruct_text(sentences[i], tokenizer_encoder, tokenizer_decoder, \n",
    "                                                            vae_models, beta_name, verbose=False)\n",
    "        print(\"Reconstructed sentence 2:\", t2)\n",
    "        if i == 10:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> bu it it it\n"
     ]
    }
   ],
   "source": [
    "reconstruct_text()\n",
    "    \n",
    "text_from_latent_code_batch(latents[\"B1.0\"], tokenizer_decoder, vae_models[\"B1.0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make t-SNE plots for different Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = 15, 10\n",
    "\n",
    "models = {beta_name: TSNE(n_components=2, random_state=0) for beta_name in [\"B1.0\", \"B0.5\", \"B0.0\"]}\n",
    "data_proj = {beta_name:None for beta_name in [\"B1.0\", \"B0.5\", \"B0.0\"]}\n",
    "for beta_name, model in models.items():\n",
    "    data = np.asarray(latents[beta_name])\n",
    "    data_proj[beta_name] = model.fit_transform(data)\n",
    "    \n",
    "colors = ['y', 'g', 'b']\n",
    "for i, (beta_name, data) in enumerate(data_proj.items()):\n",
    "    plt.scatter(data[:,0] , data[:,1], color=colors[i], label=beta_name, alpha=0.4)\n",
    "\n",
    "plt.title(\"t-SNE plot of encoded sentences by VAEs with different Beta values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
