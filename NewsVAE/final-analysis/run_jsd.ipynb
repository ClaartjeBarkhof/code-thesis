{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not deleting all pareto related files, if you want to recompute, run: update(recompute=True)\n",
      "Making run overview, based on dir: /home/cbarkhof/code-thesis/NewsVAE/Runs\n",
      "Making run overview of /home/cbarkhof/code-thesis/NewsVAE/Runs, in /home/cbarkhof/code-thesis/NewsVAE/final-analysis/Runs_run_overview.csv\n",
      "pareto epoch not in full_par_dict for run: 2021-06-02-YELP | DECODER-ONLY-run-13:01:04\n",
      "pareto epoch not in full_par_dict for run: 2021-06-02-PTB | DECODER-ONLY-run-13:01:37\n",
      "Reading last checkpoint and extracting pareto dict and saving it to a pickle.\n",
      "Reading all pareto dicts and calculating best checkpoint, saving it to a csv\n",
      "error in calc_weighted_pareto_best_checkpoint list index out of range\n",
      "error in calc_weighted_pareto_best_checkpoint list index out of range\n",
      "error in calc_weighted_pareto_best_checkpoint list index out of range\n",
      "--------------------------------------------------\n",
      "Making run overview, based on dir: /home/cbarkhof/code-thesis/NewsVAE/Runs-ablation\n",
      "Making run overview of /home/cbarkhof/code-thesis/NewsVAE/Runs-ablation, in /home/cbarkhof/code-thesis/NewsVAE/final-analysis/Runs-ablation_run_overview.csv\n",
      "Reading last checkpoint and extracting pareto dict and saving it to a pickle.\n",
      "Reading all pareto dicts and calculating best checkpoint, saving it to a csv\n",
      "--------------------------------------------------\n",
      "Making run overview, based on dir: /home/cbarkhof/code-thesis/NewsVAE/Runs-target-rate\n",
      "Making run overview of /home/cbarkhof/code-thesis/NewsVAE/Runs-target-rate, in /home/cbarkhof/code-thesis/NewsVAE/final-analysis/Runs-target-rate_run_overview.csv\n",
      "Reading last checkpoint and extracting pareto dict and saving it to a pickle.\n",
      "Reading all pareto dicts and calculating best checkpoint, saving it to a csv\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Utils analysis\n",
    "from utils_analysis import *\n",
    "\n",
    "# Standard\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys; sys.path.append(\"../\")\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(p, q, log_p, log_q):\n",
    "    \"\"\"\n",
    "    Jenson Shannon Divergence(P || Q)\n",
    "    \n",
    "    p, q: n-dimensional tensors both expected to contain log-probabilities\n",
    "    Args:\n",
    "        \n",
    "    \n",
    "    \n",
    "    they should be batch x seq_len x vocab_size\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Mean distribution \n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    \n",
    "    # JSD: symmtretical KL divergence\n",
    "    # unreduced KL: p(x) * log ( p(x) / q(x) ) -> sum over probability dimension (last dim.)\n",
    "    # kl div: input given is expected to contain log-probabilities and is not restricted to a 2D Tensor. \n",
    "    # The targets are interpreted as probabilities by default\n",
    "    jsd = 0.5 * F.kl_div(log_p, m, reduction=\"none\").sum(dim=-1) + \\\n",
    "          0.5 * F.kl_div(log_q, m, reduction=\"none\").sum(dim=-1)\n",
    "        \n",
    "    return jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_train import load_from_checkpoint, transfer_batch_to_device\n",
    "from dataset_wrappper import NewsData\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def calc_JSD_over_seq(run_name, exp_name, max_batches, batch_size, dataset, n_exp, device=\"cuda:0\"):\n",
    "    # collect the mean JSD between prior decoded and posterior decoded samples over 10 samples \n",
    "    # for the whole validation set\n",
    "    jsd_over_seq = []\n",
    "    jsd_post_post = []\n",
    "    jsd_prior_post = []\n",
    "    label_masks = []\n",
    "\n",
    "    path, best_epoch = get_best_checkpoint(run_name, exp_name=exp_name)\n",
    "    \n",
    "    if path is None:\n",
    "        return None\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    # Get data that this model was trained on\n",
    "    if \"optimus\" in dataset.lower():\n",
    "        dataset_name = \"optimus_yelp\"\n",
    "    elif \"yelp\" in dataset.lower():\n",
    "        dataset_name = \"yelp\"\n",
    "    else:\n",
    "        dataset_name = \"ptb_text_only\"\n",
    "\n",
    "    # Load relevant data\n",
    "    data = NewsData(dataset_name=dataset_name, tokenizer_name=\"roberta\", batch_size=batch_size, \n",
    "                    num_workers=3, pin_memory=True, max_seq_len=64, device=device)\n",
    "    val_loader = data.val_dataloader(shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    # Get model\n",
    "    vae_model = load_from_checkpoint(path, world_master=True, ddp=False, device_name=device, \n",
    "                                     evaluation=True, return_loss_term_manager=False)\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_i, batch in enumerate(val_loader):\n",
    "\n",
    "            batch = transfer_batch_to_device(batch, device)\n",
    "\n",
    "            logits_prior_batch = []\n",
    "            logits_posterior_batch = []\n",
    "\n",
    "            for exp_i in range(n_exp):\n",
    "                print(f\"Batch: {batch_i+1:3d}/{max_batches} - Exp: {exp_i+1:3d}/{n_exp}\", end=\"\\r\")\n",
    "\n",
    "                for decode_sample_from_prior_mode in [False, True]:\n",
    "\n",
    "                    vae_output = vae_model(input_ids=batch[\"input_ids\"],\n",
    "                                           attention_mask=batch[\"attention_mask\"],\n",
    "                                           auto_regressive=False,\n",
    "                                           max_seq_len=64,\n",
    "                                           return_reconstruction_loss=True,\n",
    "                                           return_posterior_stats=False,\n",
    "                                           nucleus_sampling=False,\n",
    "                                           top_k=0,\n",
    "                                           top_p=1.0,\n",
    "\n",
    "                                           return_logits=True,\n",
    "\n",
    "                                           # these two are the relevant ones\n",
    "                                           decode_sample_from_prior=decode_sample_from_prior_mode,\n",
    "                                           n_prior_samples=batch_size, # as many as the batch reconstruct, so batch size\n",
    "\n",
    "                                           device_name=device)\n",
    "\n",
    "                    if decode_sample_from_prior_mode is True:\n",
    "                        logits_prior_batch.append(vae_output[\"logits\"].cpu())\n",
    "                    else:\n",
    "                        logits_posterior_batch.append(vae_output[\"logits\"].cpu())\n",
    "\n",
    "            # make a reordered posterior list, to compare posterior \n",
    "            # to posterior (and account for internal variability)\n",
    "            re_order = list(np.arange(1, n_exp)) + [0]          \n",
    "            logits_posterior_reordered = [logits_posterior_batch[i] for i in re_order]            \n",
    "\n",
    "            # After cat operation, they matrices are [n_exp * batchsize x seq_len x vocab]\n",
    "            # The log soft_max should operate on the last dimension\n",
    "            log_probs_prior_batch = F.log_softmax(torch.cat(logits_prior_batch, dim=0), dim=-1)\n",
    "            log_probs_posterior_batch = F.log_softmax(torch.cat(logits_posterior_batch, dim=0), dim=-1)\n",
    "            log_probs_posterior_reordered_batch = F.log_softmax(torch.cat(logits_posterior_reordered, dim=0), dim=-1)\n",
    "\n",
    "            probs_prior_batch = F.softmax(torch.cat(logits_prior_batch, dim=0), dim=-1)\n",
    "            probs_posterior_batch = F.softmax(torch.cat(logits_posterior_batch, dim=0), dim=-1)\n",
    "            probs_posterior_reordered_batch = F.softmax(torch.cat(logits_posterior_reordered, dim=0), dim=-1)\n",
    "\n",
    "            # JSD returns [n_exp * batchsize x seq_len]\n",
    "            jsd_prior_posterior = JSD(probs_posterior_batch, probs_prior_batch, log_probs_posterior_batch, log_probs_prior_batch)\n",
    "            jsd_posterior_posterior = JSD(probs_posterior_batch, probs_posterior_reordered_batch, log_probs_posterior_batch, log_probs_posterior_reordered_batch)\n",
    "            jsd_dif = jsd_prior_posterior - jsd_posterior_posterior\n",
    "\n",
    "            # Take into account the different sequence lengths, correct for that when averaging\n",
    "            labels = batch[\"input_ids\"].cpu()[:, 1:].contiguous()  # skip <s> token\n",
    "            label_mask = (labels != 1).float().repeat(n_exp, 1) # pad token is int 1\n",
    "            #label_mask_sum_batch_exp = label_mask.sum(dim=0) # sum over the batch, n_exp dim\n",
    "            # mean_jsd_dif = jsd_dif.sum(dim=0) / label_mask_sum_batch_exp\n",
    "\n",
    "            jsd_over_seq.append(jsd_dif)\n",
    "            jsd_post_post.append(jsd_posterior_posterior)\n",
    "            jsd_prior_post.append(jsd_prior_posterior)\n",
    "            label_masks.append(label_mask)\n",
    "\n",
    "            if batch_i == max_batches - 1:\n",
    "                break\n",
    "\n",
    "            # ------- END BATCH EXP ---------\n",
    "\n",
    "        # -------- END ALL BATCHES ----------\n",
    "\n",
    "    # Maximum sequence length, needed for padding\n",
    "    max_len = max([t.shape[1] for t in jsd_over_seq])\n",
    "\n",
    "    # pad all to have the same length \n",
    "    jsd_over_seq = [F.pad(x, (0, max_len-x.shape[1])) for x in jsd_over_seq]\n",
    "    jsd_over_seq = torch.cat(jsd_over_seq, dim=0)\n",
    "#     jsd_over_seq = jsd_over_seq.mean(dim=0)\n",
    "\n",
    "    jsd_post_post = [F.pad(x, (0, max_len-x.shape[1])) for x in jsd_post_post]\n",
    "    jsd_post_post = torch.cat(jsd_post_post, dim=0)\n",
    "\n",
    "    jsd_prior_post = [F.pad(x, (0, max_len-x.shape[1])) for x in jsd_prior_post]\n",
    "    jsd_prior_post = torch.cat(jsd_prior_post, dim=0)\n",
    "\n",
    "    label_masks = [F.pad(x, (0, max_len-x.shape[1])) for x in label_masks]\n",
    "    label_masks = torch.cat(label_masks, dim=0)\n",
    "\n",
    "    results_jsd ={\n",
    "        \"jsd_over_seq\": jsd_over_seq,\n",
    "        \"jsd_post_post\": jsd_post_post,\n",
    "        \"jsd_prior_post\": jsd_prior_post,\n",
    "        \"label_masks\": label_masks\n",
    "    }\n",
    "    \n",
    "    return results_jsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "YELP | MDR-0.5 | matrix+mem | DROP 40\n",
      "2021-05-31-YELP | MDR-0.5 | matrix-memory | DROP 40-run-06:30:27\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-31-YELP | MDR-0.5 | matrix-memory | DROP 40-run-06:30:27/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-31-YELP | MDR-0.5 | matrix-memory | DROP 40-run-06:30:27/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | CYC-FB-0.5 | matrix+mem\n",
      "2021-05-24-PTB | CYC-FB-0.5 | matrix-memory-run-19:12:45\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | CYC-FB-0.5 | matrix-memory-run-19:12:45/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | CYC-FB-0.5 | matrix-memory-run-19:12:45/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | VAE | matrix\n",
      "2021-05-26-YELP | VAE | matrix-run-20:28:49\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-26-YELP | VAE | matrix-run-20:28:49/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-26-YELP | VAE | matrix-run-20:28:49/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | CYC-FB-0.5 | matrix+mem\n",
      "2021-05-30-YELP | CYC-FB-0.5 | matrix-memory-run-04:07:55\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-30-YELP | CYC-FB-0.5 | matrix-memory-run-04:07:55/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-30-YELP | CYC-FB-0.5 | matrix-memory-run-04:07:55/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | MDR-0.5 | mem+emb | DROP 40\n",
      "2021-05-31-YELP | MDR-0.5 | memory-embeddings | DROP 40-run-15:27:52\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-31-YELP | MDR-0.5 | memory-embeddings | DROP 40-run-15:27:52/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-31-YELP | MDR-0.5 | memory-embeddings | DROP 40-run-15:27:52/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | VAE | mem\n",
      "2021-05-21-PTB | VAE | memory-run-05:50:37\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-21-PTB | VAE | memory-run-05:50:37/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-21-PTB | VAE | memory-run-05:50:37/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | VAE | mem | DROP 40\n",
      "2021-05-21-PTB | VAE | memory | DROP 40-run-05:54:09\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-21-PTB | VAE | memory | DROP 40-run-05:54:09/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-21-PTB | VAE | memory | DROP 40-run-05:54:09/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | VAE | mem+emb\n",
      "2021-05-21-PTB | VAE | memory-embeddings-run-06:42:33\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-21-PTB | VAE | memory-embeddings-run-06:42:33/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-21-PTB | VAE | memory-embeddings-run-06:42:33/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | VAE | mem+emb | DROP 40\n",
      "2021-05-21-PTB | VAE | memory-embeddings | DROP 40-run-14:35:15\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-21-PTB | VAE | memory-embeddings | DROP 40-run-14:35:15/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-21-PTB | VAE | memory-embeddings | DROP 40-run-14:35:15/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | MDR-0.5 | matrix+mem\n",
      "2021-05-25-PTB | MDR-0.5 | matrix-memory-run-07:49:50\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-25-PTB | MDR-0.5 | matrix-memory-run-07:49:50/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-25-PTB | MDR-0.5 | matrix-memory-run-07:49:50/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | CYC-FB-0.5 | matrix\n",
      "2021-05-29-YELP | CYC-FB-0.5 | matrix-run-07:06:17\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-29-YELP | CYC-FB-0.5 | matrix-run-07:06:17/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-29-YELP | CYC-FB-0.5 | matrix-run-07:06:17/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | MDR-0.5 | matrix | DROP 40\n",
      "2021-05-31-YELP | MDR-0.5 | matrix | DROP 40-run-01:27:10\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-31-YELP | MDR-0.5 | matrix | DROP 40-run-01:27:10/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-31-YELP | MDR-0.5 | matrix | DROP 40-run-01:27:10/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | CYC-FB-0.5 | mem\n",
      "2021-05-22-PTB | CYC-FB-0.5 | memory-run-02:03:27\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | CYC-FB-0.5 | memory-run-02:03:27/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | CYC-FB-0.5 | memory-run-02:03:27/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | CYC-FB-0.5 | mem | DROP 40\n",
      "2021-05-22-PTB | CYC-FB-0.5 | memory | DROP 40-run-04:00:13\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | CYC-FB-0.5 | memory | DROP 40-run-04:00:13/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | CYC-FB-0.5 | memory | DROP 40-run-04:00:13/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | CYC-FB-0.5 | mem+emb\n",
      "2021-05-22-PTB | CYC-FB-0.5 | memory-embeddings-run-07:04:27\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | CYC-FB-0.5 | memory-embeddings-run-07:04:27/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | CYC-FB-0.5 | memory-embeddings-run-07:04:27/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "PTB | CYC-FB-0.5 | mem+emb | DROP 40\n",
      "2021-05-22-PTB | CYC-FB-0.5 | memory-embeddings | DROP 40-run-09:00:45\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | CYC-FB-0.5 | memory-embeddings | DROP 40-run-09:00:45/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | CYC-FB-0.5 | memory-embeddings | DROP 40-run-09:00:45/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | CYC-FB-0.5 | matrix+mem | DROP 40\n",
      "2021-05-24-PTB | CYC-FB-0.5 | matrix-memory | DROP 40-run-19:30:07\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | CYC-FB-0.5 | matrix-memory | DROP 40-run-19:30:07/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | CYC-FB-0.5 | matrix-memory | DROP 40-run-19:30:07/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | VAE | matrix+mem\n",
      "2021-05-27-YELP | VAE | matrix-memory-run-09:57:46\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-27-YELP | VAE | matrix-memory-run-09:57:46/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-27-YELP | VAE | matrix-memory-run-09:57:46/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | MDR-0.5 | matrix\n",
      "2021-05-30-YELP | MDR-0.5 | matrix-run-23:55:12\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-30-YELP | MDR-0.5 | matrix-run-23:55:12/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-30-YELP | MDR-0.5 | matrix-run-23:55:12/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | AE | mem\n",
      "2021-06-03-PTB | AE | memory-run-03:50:11\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-03-PTB | AE | memory-run-03:50:11/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-03-PTB | AE | memory-run-03:50:11/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | MDR-0.5 | mem\n",
      "2021-05-22-PTB | MDR-0.5 | memory-run-14:01:39\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | MDR-0.5 | memory-run-14:01:39/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | MDR-0.5 | memory-run-14:01:39/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | MDR-0.5 | mem | DROP 40\n",
      "2021-05-22-PTB | MDR-0.5 | memory | DROP 40-run-20:01:49\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | MDR-0.5 | memory | DROP 40-run-20:01:49/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | MDR-0.5 | memory | DROP 40-run-20:01:49/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | MDR-0.5 | mem+emb\n",
      "2021-05-22-PTB | MDR-0.5 | memory-embeddings-run-20:03:20\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | MDR-0.5 | memory-embeddings-run-20:03:20/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | MDR-0.5 | memory-embeddings-run-20:03:20/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | MDR-0.5 | mem+emb | DROP 40\n",
      "2021-05-22-PTB | MDR-0.5 | memory-embeddings | DROP 40-run-20:58:03\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | MDR-0.5 | memory-embeddings | DROP 40-run-20:58:03/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-PTB | MDR-0.5 | memory-embeddings | DROP 40-run-20:58:03/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | MDR-0.5 | matrix\n",
      "2021-05-24-PTB | MDR-0.5 | matrix-run-20:53:36\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | MDR-0.5 | matrix-run-20:53:36/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | MDR-0.5 | matrix-run-20:53:36/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | VAE | matrix | DROP 40\n",
      "2021-05-26-YELP | VAE | matrix | DROP 40-run-20:35:14\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-26-YELP | VAE | matrix | DROP 40-run-20:35:14/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-26-YELP | VAE | matrix | DROP 40-run-20:35:14/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | CYC-FB-0.5 | matrix+mem | DROP 40\n",
      "2021-05-30-YELP | CYC-FB-0.5 | matrix-memory | DROP 40-run-07:54:33\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-30-YELP | CYC-FB-0.5 | matrix-memory | DROP 40-run-07:54:33/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-30-YELP | CYC-FB-0.5 | matrix-memory | DROP 40-run-07:54:33/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | VAE | matrix+mem\n",
      "2021-06-01-PTB | VAE | matrix-memory-run-02:05:15\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-01-PTB | VAE | matrix-memory-run-02:05:15/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-01-PTB | VAE | matrix-memory-run-02:05:15/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | VAE | mem\n",
      "2021-05-22-YELP | VAE | memory-run-23:33:15\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-YELP | VAE | memory-run-23:33:15/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-22-YELP | VAE | memory-run-23:33:15/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | VAE | mem | DROP 40\n",
      "2021-05-23-YELP | VAE | memory | DROP 40-run-06:24:30\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | VAE | memory | DROP 40-run-06:24:30/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | VAE | memory | DROP 40-run-06:24:30/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "YELP | VAE | mem+emb\n",
      "2021-05-23-YELP | VAE | memory-embeddings-run-07:33:28\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | VAE | memory-embeddings-run-07:33:28/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | VAE | memory-embeddings-run-07:33:28/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | VAE | mem+emb | DROP 40\n",
      "2021-05-23-YELP | VAE | memory-embeddings | DROP 40-run-09:15:07\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | VAE | memory-embeddings | DROP 40-run-09:15:07/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | VAE | memory-embeddings | DROP 40-run-09:15:07/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | MDR-0.5 | matrix | DROP 40\n",
      "2021-05-24-PTB | MDR-0.5 | matrix | DROP 40-run-21:32:54\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | MDR-0.5 | matrix | DROP 40-run-21:32:54/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | MDR-0.5 | matrix | DROP 40-run-21:32:54/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | VAE | matrix+mem | DROP 40\n",
      "2021-05-28-YELP | VAE | matrix-memory | DROP 40-run-04:12:08\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-28-YELP | VAE | matrix-memory | DROP 40-run-04:12:08/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-28-YELP | VAE | matrix-memory | DROP 40-run-04:12:08/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | MDR-0.5 | matrix+mem\n",
      "2021-05-31-YELP | MDR-0.5 | matrix-memory-run-05:56:38\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-31-YELP | MDR-0.5 | matrix-memory-run-05:56:38/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-31-YELP | MDR-0.5 | matrix-memory-run-05:56:38/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | CYC-FB-0.5 | mem\n",
      "2021-05-23-YELP | CYC-FB-0.5 | memory-run-10:56:31\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | CYC-FB-0.5 | memory-run-10:56:31/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | CYC-FB-0.5 | memory-run-10:56:31/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | CYC-FB-0.5 | mem | DROP 40\n",
      "2021-05-23-YELP | CYC-FB-0.5 | memory | DROP 40-run-17:15:59\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | CYC-FB-0.5 | memory | DROP 40-run-17:15:59/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | CYC-FB-0.5 | memory | DROP 40-run-17:15:59/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | CYC-FB-0.5 | mem+emb\n",
      "2021-05-23-YELP | CYC-FB-0.5 | memory-embeddings-run-18:19:10\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | CYC-FB-0.5 | memory-embeddings-run-18:19:10/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | CYC-FB-0.5 | memory-embeddings-run-18:19:10/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | CYC-FB-0.5 | mem+emb | DROP 40\n",
      "2021-05-23-YELP | CYC-FB-0.5 | memory-embeddings | DROP 40-run-18:57:08\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | CYC-FB-0.5 | memory-embeddings | DROP 40-run-18:57:08/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | CYC-FB-0.5 | memory-embeddings | DROP 40-run-18:57:08/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | MDR-0.5 | mem\n",
      "2021-05-23-YELP | MDR-0.5 | memory-run-19:38:13\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | MDR-0.5 | memory-run-19:38:13/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | MDR-0.5 | memory-run-19:38:13/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | MDR-0.5 | mem | DROP 40\n",
      "2021-05-23-YELP | MDR-0.5 | memory | DROP 40-run-20:10:35\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | MDR-0.5 | memory | DROP 40-run-20:10:35/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | MDR-0.5 | memory | DROP 40-run-20:10:35/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | MDR-0.5 | mem+emb\n",
      "2021-05-23-YELP | MDR-0.5 | memory-embeddings-run-20:52:29\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | MDR-0.5 | memory-embeddings-run-20:52:29/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-23-YELP | MDR-0.5 | memory-embeddings-run-20:52:29/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | VAE | matrix\n",
      "2021-05-24-PTB | VAE | matrix-run-11:05:20\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | VAE | matrix-run-11:05:20/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | VAE | matrix-run-11:05:20/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | VAE | matrix | DROP 40\n",
      "2021-05-24-PTB | VAE | matrix | DROP 40-run-12:08:05\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | VAE | matrix | DROP 40-run-12:08:05/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | VAE | matrix | DROP 40-run-12:08:05/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | CYC-FB-0.5 | matrix\n",
      "2021-05-31-PTB | CYC-FB-0.5 | matrix-run-17:33:15\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-31-PTB | CYC-FB-0.5 | matrix-run-17:33:15/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-31-PTB | CYC-FB-0.5 | matrix-run-17:33:15/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "PTB | VAE | matrix+mem | DROP 40\n",
      "2021-05-24-PTB | VAE | matrix-memory | DROP 40-run-15:34:33\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | VAE | matrix-memory | DROP 40-run-15:34:33/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-24-PTB | VAE | matrix-memory | DROP 40-run-15:34:33/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | MDR-0.5 | matrix+mem | DROP 40\n",
      "2021-05-26-PTB | MDR-0.5 | matrix-memory | DROP 40-run-09:00:48\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-26-PTB | MDR-0.5 | matrix-memory | DROP 40-run-09:00:48/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-26-PTB | MDR-0.5 | matrix-memory | DROP 40-run-09:00:48/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | CYC-FB-0.5 | matrix | DROP 40\n",
      "2021-05-29-YELP | CYC-FB-0.5 | matrix | DROP 40-run-08:07:49\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-29-YELP | CYC-FB-0.5 | matrix | DROP 40-run-08:07:49/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-05-29-YELP | CYC-FB-0.5 | matrix | DROP 40-run-08:07:49/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | CYC-FB-0.5 | matrix | DROP 40\n",
      "2021-06-01-PTB | CYC-FB-0.5 | matrix | DROP 40-run-03:33:38\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-01-PTB | CYC-FB-0.5 | matrix | DROP 40-run-03:33:38/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-01-PTB | CYC-FB-0.5 | matrix | DROP 40-run-03:33:38/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | DEC-ONLY | \n",
      "2021-06-02-YELP | DECODER-ONLY-run-13:01:04\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-02-YELP | DECODER-ONLY-run-13:01:04/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "**************************************************\n",
      "PTB | AE | matrix\n",
      "2021-06-04-PTB | AE | matrix-run-02:44:35\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-04-PTB | AE | matrix-run-02:44:35/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Is file!\n",
      "train 42068\n",
      "validation 3370\n",
      "test 3761\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-04-PTB | AE | matrix-run-02:44:35/checkpoint-epoch-007-step-8416-iw-ll_462.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.alpha', 'roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.alpha', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.alpha', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.alpha', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.alpha', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.alpha', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.alpha', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.alpha', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.alpha', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.alpha', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.alpha', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.alpha', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.alpha', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.alpha', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.alpha', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.alpha', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.alpha', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.alpha', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.alpha', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.alpha', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.alpha', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.alpha', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.alpha', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.alpha', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.alpha', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.alpha', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.alpha', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.alpha', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.alpha', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.alpha', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.alpha', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.alpha', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.alpha', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.alpha', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.alpha', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.alpha', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "Checkpoint global_step: 8416, epoch: 7\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "Batch:   1/4 - Exp:   1/5\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../loss_and_optimisation.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.exp(-torch.tensor(kernel_input))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "PTB | AE | emb\n",
      "2021-06-02-PTB | AE | embeddings-run-03:33:15\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-02-PTB | AE | embeddings-run-03:33:15/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-02-PTB | AE | embeddings-run-03:33:15/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | AE | mem+emb\n",
      "2021-06-02-YELP | AE | memory-embeddings-run-13:01:03\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-02-YELP | AE | memory-embeddings-run-13:01:03/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-02-YELP | AE | memory-embeddings-run-13:01:03/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | DEC-ONLY | \n",
      "2021-06-02-PTB | DECODER-ONLY-run-13:01:37\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-02-PTB | DECODER-ONLY-run-13:01:37/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "**************************************************\n",
      "PTB | AE | mem+emb\n",
      "2021-06-05-PTB | AE | memory-embeddings-run-05:11:41\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-05-PTB | AE | memory-embeddings-run-05:11:41/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Is file!\n",
      "train 42068\n",
      "validation 3370\n",
      "test 3761\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-05-PTB | AE | memory-embeddings-run-05:11:41/checkpoint-epoch-017-step-5922-iw-ll_219.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "Checkpoint global_step: 5922, epoch: 17\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "**************************************************\n",
      "YELP | AE | matrix\n",
      "2021-06-05-YELP | AE | matrix-run-07:16:22\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-05-YELP | AE | matrix-run-07:16:22/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Is file!\n",
      "train 650000\n",
      "validation 25000\n",
      "test 25000\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-05-YELP | AE | matrix-run-07:16:22/checkpoint-epoch-009-step-100010-iw-ll_550.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.alpha', 'roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.alpha', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.alpha', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.alpha', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.alpha', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.alpha', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.alpha', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.alpha', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.alpha', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.alpha', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.alpha', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.alpha', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.alpha', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.alpha', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.alpha', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.alpha', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.alpha', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.alpha', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.alpha', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.alpha', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.alpha', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.alpha', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.alpha', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.alpha', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.alpha', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.alpha', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.alpha', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.alpha', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.alpha', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.alpha', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.alpha', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.alpha', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.alpha', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.alpha', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.alpha', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.alpha', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "Checkpoint global_step: 100010, epoch: 9\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "**************************************************\n",
      "YELP | AE | matrix+mem\n",
      "2021-06-06-YELP | AE | matrix-memory-run-02:54:57\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-06-YELP | AE | matrix-memory-run-02:54:57/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Is file!\n",
      "train 650000\n",
      "validation 25000\n",
      "test 25000\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-06-YELP | AE | matrix-memory-run-02:54:57/checkpoint-epoch-009-step-100010-iw-ll_496.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.alpha', 'roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.alpha', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.alpha', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.alpha', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.alpha', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.alpha', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.alpha', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.alpha', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.alpha', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.alpha', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.alpha', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.alpha', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.alpha', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.alpha', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.alpha', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.alpha', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.alpha', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.alpha', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.alpha', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.alpha', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.alpha', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.alpha', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.alpha', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.alpha', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.alpha', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.alpha', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.alpha', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.alpha', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.alpha', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.alpha', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.alpha', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.alpha', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.alpha', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.alpha', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.alpha', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.alpha', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "Checkpoint global_step: 100010, epoch: 9\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "**************************************************\n",
      "YELP | AE | emb\n",
      "2021-06-06-YELP | AE | embeddings-run-05:09:31\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-06-YELP | AE | embeddings-run-05:09:31/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Is file!\n",
      "train 650000\n",
      "validation 25000\n",
      "test 25000\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-06-YELP | AE | embeddings-run-05:09:31/checkpoint-epoch-009-step-50790-iw-ll_411.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "Checkpoint global_step: 50790, epoch: 9\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "**************************************************\n",
      "YELP | AE | mem\n",
      "2021-06-06-YELP | AE | memory-run-15:12:30\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-06-YELP | AE | memory-run-15:12:30/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Is file!\n",
      "train 650000\n",
      "validation 25000\n",
      "test 25000\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-06-YELP | AE | memory-run-15:12:30/checkpoint-epoch-009-step-50790-iw-ll_442.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "Checkpoint global_step: 50790, epoch: 9\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "**************************************************\n",
      "YELP | AE | mem+emb\n",
      "2021-06-06-YELP | AE | memory-embeddings-run-15:14:38\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-06-YELP | AE | memory-embeddings-run-15:14:38/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Is file!\n",
      "train 650000\n",
      "validation 25000\n",
      "test 25000\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-06-YELP | AE | memory-embeddings-run-15:14:38/checkpoint-epoch-009-step-50790-iw-ll_390.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "Checkpoint global_step: 50790, epoch: 9\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "**************************************************\n",
      "PTB | MDR-0.5 | mem\n",
      "2021-06-07-PTB | MDR-0.5 | memory-run-07:46:12\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-07-PTB | MDR-0.5 | memory-run-07:46:12/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Is file!\n",
      "train 42068\n",
      "validation 3370\n",
      "test 3761\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-07-PTB | MDR-0.5 | memory-run-07:46:12/checkpoint-epoch-017-step-5922-iw-ll_091.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "Checkpoint global_step: 5922, epoch: 17\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "**************************************************\n",
      "PTB | MDR-0.5 | mem | DROP 40\n",
      "2021-06-07-PTB | MDR-0.5 | memory | DROP 40-run-14:54:06\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-07-PTB | MDR-0.5 | memory | DROP 40-run-14:54:06/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Is file!\n",
      "train 42068\n",
      "validation 3370\n",
      "test 3761\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-07-PTB | MDR-0.5 | memory | DROP 40-run-14:54:06/checkpoint-epoch-019-step-6580-iw-ll_090.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "Checkpoint global_step: 6580, epoch: 19\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "**************************************************\n",
      "PTB | AE | matrix+mem\n",
      "2021-06-07-PTB | AE | matrix-memory-run-22:34:56\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs/2021-06-07-PTB | AE | matrix-memory-run-22:34:56/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "**************************************************\n",
      "YELP | CYC+FB-0.5 | mem\n",
      "2021-05-17-17-MAY-yelp-cyclical-free-bits-0.5-run-11:45:29\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-ablation/2021-05-17-17-MAY-yelp-cyclical-free-bits-0.5-run-11:45:29/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-ablation/2021-05-17-17-MAY-yelp-cyclical-free-bits-0.5-run-11:45:29/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | CYC | mem\n",
      "2021-05-18-18-MAY-yelp-cyclical-only-run-11:39:28\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-ablation/2021-05-18-18-MAY-yelp-cyclical-only-run-11:39:28/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-ablation/2021-05-18-18-MAY-yelp-cyclical-only-run-11:39:28/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | FB-0.5 | mem\n",
      "2021-05-19-18-MAY-yelp-free-bits-0.5-only-run-03:00:33\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-ablation/2021-05-19-18-MAY-yelp-free-bits-0.5-only-run-03:00:33/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-ablation/2021-05-19-18-MAY-yelp-free-bits-0.5-only-run-03:00:33/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "YELP | MDR-0.5 | mem\n",
      "2021-05-19-18-MAY-yelp-mem-mdr-0.5-run-03:39:45\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-ablation/2021-05-19-18-MAY-yelp-mem-mdr-0.5-run-03:39:45/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-ablation/2021-05-19-18-MAY-yelp-mem-mdr-0.5-run-03:39:45/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | FB | mem+emb | Target rate: 0.25\n",
      "2021-05-20-rate-exp-fb-0.25-ptb-mem-emb-run-12:09:00\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-target-rate/2021-05-20-rate-exp-fb-0.25-ptb-mem-emb-run-12:09:00/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-target-rate/2021-05-20-rate-exp-fb-0.25-ptb-mem-emb-run-12:09:00/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | FB | mem+emb | Target rate: 0.5\n",
      "2021-05-20-rate-exp-fb-0.5-ptb-mem-emb-run-22:06:34\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-target-rate/2021-05-20-rate-exp-fb-0.5-ptb-mem-emb-run-22:06:34/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-target-rate/2021-05-20-rate-exp-fb-0.5-ptb-mem-emb-run-22:06:34/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | FB | mem+emb | Target rate: 0.125\n",
      "2021-05-21-rate-exp-fb-0.125-ptb-mem-emb-run-15:25:21\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-target-rate/2021-05-21-rate-exp-fb-0.125-ptb-mem-emb-run-15:25:21/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-target-rate/2021-05-21-rate-exp-fb-0.125-ptb-mem-emb-run-15:25:21/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | FB | mem+emb | Target rate: 1.0\n",
      "2021-05-21-rate-exp-fb-1.0-ptb-mem-emb-run-06:58:25\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-target-rate/2021-05-21-rate-exp-fb-1.0-ptb-mem-emb-run-06:58:25/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-target-rate/2021-05-21-rate-exp-fb-1.0-ptb-mem-emb-run-06:58:25/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "**************************************************\n",
      "PTB | FB | mem+emb | Target rate: 0.75\n",
      "2021-05-22-rate-exp-fb-0.75-ptb-mem-emb-run-12:57:25\n",
      "**************************************************\n",
      "/home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-target-rate/2021-05-22-rate-exp-fb-0.75-ptb-mem-emb-run-12:57:25/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle\n",
      "Loading file /home/cbarkhof/code-thesis/NewsVAE/final-analysis/result-files/Runs-target-rate/2021-05-22-rate-exp-fb-0.75-ptb-mem-emb-run-12:57:25/result_JSD_over_seq_N_EXP_5_MAX_BATCHES_4_BS_64.pickle, it existed.\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Failed runs\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "2021-06-02-YELP | DECODER-ONLY-run-13:01:04\n",
      "2021-06-02-PTB | DECODER-ONLY-run-13:01:37\n",
      "2021-06-07-PTB | AE | matrix-memory-run-22:34:56\n"
     ]
    }
   ],
   "source": [
    "N_EXP = 5\n",
    "MAX_BATCHES = 4\n",
    "BS = 64\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "failed_runs = []\n",
    "for exp_name, run_dir in RUN_DIRS.items():\n",
    "    run_overview = read_overview_csv(exp_name=exp_name)\n",
    "    \n",
    "    for row_i, row in run_overview.iterrows():\n",
    "        run_name, clean_name = row['run_name'], row['clean_name']\n",
    "        \n",
    "        if check_if_running(run_name, exp_name):\n",
    "            continue\n",
    "        \n",
    "        print(\"*\" * 50)\n",
    "        print(clean_name)\n",
    "        print(run_name)\n",
    "        print(\"*\" * 50)\n",
    "        \n",
    "        d = f\"{RES_FILE_DIR}/{exp_name}/{run_name}\"\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "        RESULT_FILE = f\"{d}/result_JSD_over_seq_N_EXP_{N_EXP}_MAX_BATCHES_{MAX_BATCHES}_BS_{BS}.pickle\"\n",
    "        print(RESULT_FILE)\n",
    "\n",
    "        # If already ran, do not run again\n",
    "        if os.path.exists(RESULT_FILE):\n",
    "            print(f\"Loading file {RESULT_FILE}, it existed.\")\n",
    "            results_jsd = pickle.load( open( RESULT_FILE, \"rb\" ) )\n",
    "\n",
    "        else:\n",
    "            # run_name, exp_name, max_batches, batch_size, dataset, n_exp, device=\"cuda:0\"\n",
    "            results_jsd = calc_JSD_over_seq(run_name=run_name, exp_name=exp_name, max_batches=MAX_BATCHES, batch_size=BS, dataset=row[\"dataset\"], n_exp=N_EXP, device=DEVICE)\n",
    "        \n",
    "        if results_jsd is not None:\n",
    "            pickle.dump( results_jsd, open( RESULT_FILE, \"wb\" ))\n",
    "        \n",
    "        else:\n",
    "            failed_runs.append(run_name)\n",
    "\n",
    "print(\"X\"*80)\n",
    "print(\"Failed runs\")\n",
    "print(\"X\"*80)\n",
    "for r in failed_runs:\n",
    "    print(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "thesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
