{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not deleting all pareto related files, if you want to recompute, run: update(recompute=True)\n",
      "Making run overview, based on dir: /home/cbarkhof/code-thesis/NewsVAE/Runs\n",
      "Making run overview of /home/cbarkhof/code-thesis/NewsVAE/Runs, in /home/cbarkhof/code-thesis/NewsVAE/final-analysis/Runs_run_overview.csv\n",
      "pareto epoch not in full_par_dict for run: 2021-06-02-YELP | DECODER-ONLY-run-13:01:04\n",
      "pareto epoch not in full_par_dict for run: 2021-06-02-PTB | DECODER-ONLY-run-13:01:37\n",
      "Reading last checkpoint and extracting pareto dict and saving it to a pickle.\n",
      "Reading all pareto dicts and calculating best checkpoint, saving it to a csv\n",
      "error in calc_weighted_pareto_best_checkpoint list index out of range\n",
      "error in calc_weighted_pareto_best_checkpoint list index out of range\n",
      "error in calc_weighted_pareto_best_checkpoint list index out of range\n",
      "--------------------------------------------------\n",
      "Making run overview, based on dir: /home/cbarkhof/code-thesis/NewsVAE/Runs-ablation\n",
      "Making run overview of /home/cbarkhof/code-thesis/NewsVAE/Runs-ablation, in /home/cbarkhof/code-thesis/NewsVAE/final-analysis/Runs-ablation_run_overview.csv\n",
      "Reading last checkpoint and extracting pareto dict and saving it to a pickle.\n",
      "Reading all pareto dicts and calculating best checkpoint, saving it to a csv\n",
      "--------------------------------------------------\n",
      "Making run overview, based on dir: /home/cbarkhof/code-thesis/NewsVAE/Runs-target-rate\n",
      "Making run overview of /home/cbarkhof/code-thesis/NewsVAE/Runs-target-rate, in /home/cbarkhof/code-thesis/NewsVAE/final-analysis/Runs-target-rate_run_overview.csv\n",
      "Reading last checkpoint and extracting pareto dict and saving it to a pickle.\n",
      "Reading all pareto dicts and calculating best checkpoint, saving it to a csv\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append(\"../\")\n",
    "from utils_train import load_from_checkpoint, transfer_batch_to_device\n",
    "from utils_analysis import *\n",
    "from dataset_wrappper import NewsData\n",
    "import torch\n",
    "from loss_and_optimisation import approximate_log_q_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "IW_N_SAMPLES = 200\n",
    "VAL_BATCHES = 10\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXP: Runs\n",
      "**************************************************\n",
      "2021-05-31-YELP | MDR-0.5 | matrix-memory | DROP 40-run-06:30:27\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-24-PTB | CYC-FB-0.5 | matrix-memory-run-19:12:45\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-26-YELP | VAE | matrix-run-20:28:49\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-30-YELP | CYC-FB-0.5 | matrix-memory-run-04:07:55\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-31-YELP | MDR-0.5 | memory-embeddings | DROP 40-run-15:27:52\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-21-PTB | VAE | memory-run-05:50:37\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-21-PTB | VAE | memory | DROP 40-run-05:54:09\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-21-PTB | VAE | memory-embeddings-run-06:42:33\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-21-PTB | VAE | memory-embeddings | DROP 40-run-14:35:15\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-25-PTB | MDR-0.5 | matrix-memory-run-07:49:50\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-29-YELP | CYC-FB-0.5 | matrix-run-07:06:17\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-31-YELP | MDR-0.5 | matrix | DROP 40-run-01:27:10\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-22-PTB | CYC-FB-0.5 | memory-run-02:03:27\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-22-PTB | CYC-FB-0.5 | memory | DROP 40-run-04:00:13\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-22-PTB | CYC-FB-0.5 | memory-embeddings-run-07:04:27\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-22-PTB | CYC-FB-0.5 | memory-embeddings | DROP 40-run-09:00:45\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-24-PTB | CYC-FB-0.5 | matrix-memory | DROP 40-run-19:30:07\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-27-YELP | VAE | matrix-memory-run-09:57:46\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-30-YELP | MDR-0.5 | matrix-run-23:55:12\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-06-03-PTB | AE | memory-run-03:50:11\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-22-PTB | MDR-0.5 | memory-run-14:01:39\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-22-PTB | MDR-0.5 | memory | DROP 40-run-20:01:49\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-22-PTB | MDR-0.5 | memory-embeddings-run-20:03:20\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-22-PTB | MDR-0.5 | memory-embeddings | DROP 40-run-20:58:03\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-24-PTB | MDR-0.5 | matrix-run-20:53:36\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-26-YELP | VAE | matrix | DROP 40-run-20:35:14\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-30-YELP | CYC-FB-0.5 | matrix-memory | DROP 40-run-07:54:33\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-06-01-PTB | VAE | matrix-memory-run-02:05:15\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-22-YELP | VAE | memory-run-23:33:15\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-23-YELP | VAE | memory | DROP 40-run-06:24:30\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-23-YELP | VAE | memory-embeddings-run-07:33:28\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-23-YELP | VAE | memory-embeddings | DROP 40-run-09:15:07\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-24-PTB | MDR-0.5 | matrix | DROP 40-run-21:32:54\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-28-YELP | VAE | matrix-memory | DROP 40-run-04:12:08\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-31-YELP | MDR-0.5 | matrix-memory-run-05:56:38\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-23-YELP | CYC-FB-0.5 | memory-run-10:56:31\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-23-YELP | CYC-FB-0.5 | memory | DROP 40-run-17:15:59\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-23-YELP | CYC-FB-0.5 | memory-embeddings-run-18:19:10\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-23-YELP | CYC-FB-0.5 | memory-embeddings | DROP 40-run-18:57:08\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-23-YELP | MDR-0.5 | memory-run-19:38:13\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-23-YELP | MDR-0.5 | memory | DROP 40-run-20:10:35\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-23-YELP | MDR-0.5 | memory-embeddings-run-20:52:29\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-24-PTB | VAE | matrix-run-11:05:20\n",
      "**************************************************\n",
      "Did this one already, continuing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "2021-05-24-PTB | VAE | matrix | DROP 40-run-12:08:05\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-31-PTB | CYC-FB-0.5 | matrix-run-17:33:15\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-24-PTB | VAE | matrix-memory | DROP 40-run-15:34:33\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-26-PTB | MDR-0.5 | matrix-memory | DROP 40-run-09:00:48\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-05-29-YELP | CYC-FB-0.5 | matrix | DROP 40-run-08:07:49\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-06-01-PTB | CYC-FB-0.5 | matrix | DROP 40-run-03:33:38\n",
      "**************************************************\n",
      "Did this one already, continuing.\n",
      "**************************************************\n",
      "2021-06-02-YELP | DECODER-ONLY-run-13:01:04\n",
      "**************************************************\n",
      "Is file!\n",
      "train 650000\n",
      "validation 25000\n",
      "test 25000\n",
      "** ERROR for : 2021-06-02-YELP | DECODER-ONLY-run-13:01:04 stat: path should be string, bytes, os.PathLike or integer, not NoneType\n",
      "**************************************************\n",
      "2021-06-04-PTB | AE | matrix-run-02:44:35\n",
      "**************************************************\n",
      "Is file!\n",
      "train 42068\n",
      "validation 3370\n",
      "test 3761\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-04-PTB | AE | matrix-run-02:44:35/checkpoint-epoch-007-step-8416-iw-ll_462.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.alpha', 'roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.alpha', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.alpha', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.alpha', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.alpha', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.alpha', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.alpha', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.alpha', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.alpha', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.alpha', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.alpha', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.alpha', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.alpha', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.alpha', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.alpha', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.alpha', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.alpha', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.alpha', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.alpha', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.alpha', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.alpha', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.alpha', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.alpha', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.alpha', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.alpha', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.alpha', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.alpha', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.alpha', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.alpha', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.alpha', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.alpha', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.alpha', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.alpha', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.alpha', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.alpha', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.alpha', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "********************************************************************************\n",
      "Free bits pd 0.0\n",
      "********************************************************************************\n",
      "Checkpoint global_step: 8416, epoch: 7\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "Batch   1/ 10\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../loss_and_optimisation.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.exp(-torch.tensor(kernel_input))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "2021-06-02-PTB | AE | embeddings-run-03:33:15\n",
      "**************************************************\n",
      "Is file!\n",
      "train 42068\n",
      "validation 3370\n",
      "test 3761\n",
      "** ERROR for : 2021-06-02-PTB | AE | embeddings-run-03:33:15 stat: path should be string, bytes, os.PathLike or integer, not NoneType\n",
      "**************************************************\n",
      "2021-06-02-YELP | AE | memory-embeddings-run-13:01:03\n",
      "**************************************************\n",
      "Is file!\n",
      "train 650000\n",
      "validation 25000\n",
      "test 25000\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-02-YELP | AE | memory-embeddings-run-13:01:03/checkpoint-epoch-009-step-50790-iw-ll_390.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "********************************************************************************\n",
      "Free bits pd 0.0\n",
      "********************************************************************************\n",
      "Checkpoint global_step: 50790, epoch: 9\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "**************************************************\n",
      "2021-06-02-PTB | DECODER-ONLY-run-13:01:37\n",
      "**************************************************\n",
      "Is file!\n",
      "train 42068\n",
      "validation 3370\n",
      "test 3761\n",
      "** ERROR for : 2021-06-02-PTB | DECODER-ONLY-run-13:01:37 stat: path should be string, bytes, os.PathLike or integer, not NoneType\n",
      "**************************************************\n",
      "2021-06-05-PTB | AE | memory-embeddings-run-05:11:41\n",
      "**************************************************\n",
      "Is file!\n",
      "train 42068\n",
      "validation 3370\n",
      "test 3761\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-05-PTB | AE | memory-embeddings-run-05:11:41/checkpoint-epoch-017-step-5922-iw-ll_219.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "********************************************************************************\n",
      "Free bits pd 0.0\n",
      "********************************************************************************\n",
      "Checkpoint global_step: 5922, epoch: 17\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "**************************************************\n",
      "2021-06-05-YELP | AE | matrix-run-07:16:22\n",
      "**************************************************\n",
      "Is file!\n",
      "train 650000\n",
      "validation 25000\n",
      "test 25000\n",
      "Loading model from checkpoint: /home/cbarkhof/code-thesis/NewsVAE/Runs/2021-06-05-YELP | AE | matrix-run-07:16:22/checkpoint-epoch-009-step-100010-iw-ll_550.pth\n",
      "found a config in the checkpoint!\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VaeDecoderRobertaForCausalLM: ['lm_head.bias']\n",
      "- This IS expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VaeDecoderRobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VaeDecoderRobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.query_module.alpha', 'roberta.encoder.layer.0.attention.self.query_module.layer.weight', 'roberta.encoder.layer.0.attention.self.query_module.layer.bias', 'roberta.encoder.layer.0.attention.self.key_module.alpha', 'roberta.encoder.layer.0.attention.self.key_module.layer.weight', 'roberta.encoder.layer.0.attention.self.key_module.layer.bias', 'roberta.encoder.layer.0.attention.self.value_module.alpha', 'roberta.encoder.layer.0.attention.self.value_module.layer.weight', 'roberta.encoder.layer.0.attention.self.value_module.layer.bias', 'roberta.encoder.layer.1.attention.self.query_module.alpha', 'roberta.encoder.layer.1.attention.self.query_module.layer.weight', 'roberta.encoder.layer.1.attention.self.query_module.layer.bias', 'roberta.encoder.layer.1.attention.self.key_module.alpha', 'roberta.encoder.layer.1.attention.self.key_module.layer.weight', 'roberta.encoder.layer.1.attention.self.key_module.layer.bias', 'roberta.encoder.layer.1.attention.self.value_module.alpha', 'roberta.encoder.layer.1.attention.self.value_module.layer.weight', 'roberta.encoder.layer.1.attention.self.value_module.layer.bias', 'roberta.encoder.layer.2.attention.self.query_module.alpha', 'roberta.encoder.layer.2.attention.self.query_module.layer.weight', 'roberta.encoder.layer.2.attention.self.query_module.layer.bias', 'roberta.encoder.layer.2.attention.self.key_module.alpha', 'roberta.encoder.layer.2.attention.self.key_module.layer.weight', 'roberta.encoder.layer.2.attention.self.key_module.layer.bias', 'roberta.encoder.layer.2.attention.self.value_module.alpha', 'roberta.encoder.layer.2.attention.self.value_module.layer.weight', 'roberta.encoder.layer.2.attention.self.value_module.layer.bias', 'roberta.encoder.layer.3.attention.self.query_module.alpha', 'roberta.encoder.layer.3.attention.self.query_module.layer.weight', 'roberta.encoder.layer.3.attention.self.query_module.layer.bias', 'roberta.encoder.layer.3.attention.self.key_module.alpha', 'roberta.encoder.layer.3.attention.self.key_module.layer.weight', 'roberta.encoder.layer.3.attention.self.key_module.layer.bias', 'roberta.encoder.layer.3.attention.self.value_module.alpha', 'roberta.encoder.layer.3.attention.self.value_module.layer.weight', 'roberta.encoder.layer.3.attention.self.value_module.layer.bias', 'roberta.encoder.layer.4.attention.self.query_module.alpha', 'roberta.encoder.layer.4.attention.self.query_module.layer.weight', 'roberta.encoder.layer.4.attention.self.query_module.layer.bias', 'roberta.encoder.layer.4.attention.self.key_module.alpha', 'roberta.encoder.layer.4.attention.self.key_module.layer.weight', 'roberta.encoder.layer.4.attention.self.key_module.layer.bias', 'roberta.encoder.layer.4.attention.self.value_module.alpha', 'roberta.encoder.layer.4.attention.self.value_module.layer.weight', 'roberta.encoder.layer.4.attention.self.value_module.layer.bias', 'roberta.encoder.layer.5.attention.self.query_module.alpha', 'roberta.encoder.layer.5.attention.self.query_module.layer.weight', 'roberta.encoder.layer.5.attention.self.query_module.layer.bias', 'roberta.encoder.layer.5.attention.self.key_module.alpha', 'roberta.encoder.layer.5.attention.self.key_module.layer.weight', 'roberta.encoder.layer.5.attention.self.key_module.layer.bias', 'roberta.encoder.layer.5.attention.self.value_module.alpha', 'roberta.encoder.layer.5.attention.self.value_module.layer.weight', 'roberta.encoder.layer.5.attention.self.value_module.layer.bias', 'roberta.encoder.layer.6.attention.self.query_module.alpha', 'roberta.encoder.layer.6.attention.self.query_module.layer.weight', 'roberta.encoder.layer.6.attention.self.query_module.layer.bias', 'roberta.encoder.layer.6.attention.self.key_module.alpha', 'roberta.encoder.layer.6.attention.self.key_module.layer.weight', 'roberta.encoder.layer.6.attention.self.key_module.layer.bias', 'roberta.encoder.layer.6.attention.self.value_module.alpha', 'roberta.encoder.layer.6.attention.self.value_module.layer.weight', 'roberta.encoder.layer.6.attention.self.value_module.layer.bias', 'roberta.encoder.layer.7.attention.self.query_module.alpha', 'roberta.encoder.layer.7.attention.self.query_module.layer.weight', 'roberta.encoder.layer.7.attention.self.query_module.layer.bias', 'roberta.encoder.layer.7.attention.self.key_module.alpha', 'roberta.encoder.layer.7.attention.self.key_module.layer.weight', 'roberta.encoder.layer.7.attention.self.key_module.layer.bias', 'roberta.encoder.layer.7.attention.self.value_module.alpha', 'roberta.encoder.layer.7.attention.self.value_module.layer.weight', 'roberta.encoder.layer.7.attention.self.value_module.layer.bias', 'roberta.encoder.layer.8.attention.self.query_module.alpha', 'roberta.encoder.layer.8.attention.self.query_module.layer.weight', 'roberta.encoder.layer.8.attention.self.query_module.layer.bias', 'roberta.encoder.layer.8.attention.self.key_module.alpha', 'roberta.encoder.layer.8.attention.self.key_module.layer.weight', 'roberta.encoder.layer.8.attention.self.key_module.layer.bias', 'roberta.encoder.layer.8.attention.self.value_module.alpha', 'roberta.encoder.layer.8.attention.self.value_module.layer.weight', 'roberta.encoder.layer.8.attention.self.value_module.layer.bias', 'roberta.encoder.layer.9.attention.self.query_module.alpha', 'roberta.encoder.layer.9.attention.self.query_module.layer.weight', 'roberta.encoder.layer.9.attention.self.query_module.layer.bias', 'roberta.encoder.layer.9.attention.self.key_module.alpha', 'roberta.encoder.layer.9.attention.self.key_module.layer.weight', 'roberta.encoder.layer.9.attention.self.key_module.layer.bias', 'roberta.encoder.layer.9.attention.self.value_module.alpha', 'roberta.encoder.layer.9.attention.self.value_module.layer.weight', 'roberta.encoder.layer.9.attention.self.value_module.layer.bias', 'roberta.encoder.layer.10.attention.self.query_module.alpha', 'roberta.encoder.layer.10.attention.self.query_module.layer.weight', 'roberta.encoder.layer.10.attention.self.query_module.layer.bias', 'roberta.encoder.layer.10.attention.self.key_module.alpha', 'roberta.encoder.layer.10.attention.self.key_module.layer.weight', 'roberta.encoder.layer.10.attention.self.key_module.layer.bias', 'roberta.encoder.layer.10.attention.self.value_module.alpha', 'roberta.encoder.layer.10.attention.self.value_module.layer.weight', 'roberta.encoder.layer.10.attention.self.value_module.layer.bias', 'roberta.encoder.layer.11.attention.self.query_module.alpha', 'roberta.encoder.layer.11.attention.self.query_module.layer.weight', 'roberta.encoder.layer.11.attention.self.query_module.layer.bias', 'roberta.encoder.layer.11.attention.self.key_module.alpha', 'roberta.encoder.layer.11.attention.self.key_module.layer.weight', 'roberta.encoder.layer.11.attention.self.key_module.layer.bias', 'roberta.encoder.layer.11.attention.self.value_module.alpha', 'roberta.encoder.layer.11.attention.self.value_module.layer.weight', 'roberta.encoder.layer.11.attention.self.value_module.layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done loading model...\n",
      "********************************************************************************\n",
      "Free bits pd 0.0\n",
      "********************************************************************************\n",
      "Checkpoint global_step: 100010, epoch: 9\n",
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Setting to eval mode.\n",
      "Batch   7/ 10\r"
     ]
    }
   ],
   "source": [
    "for exp_name, run_dir in RUN_DIRS.items():\n",
    "    \n",
    "    print(\"EXP:\", exp_name)\n",
    "    \n",
    "    runs_df = read_overview_csv(exp_name=exp_name)\n",
    "    \n",
    "    for row_idx, row in runs_df.iterrows():\n",
    "        run_name = row[\"run_name\"]\n",
    "\n",
    "        if check_if_running(run_name, exp_name):\n",
    "            continue\n",
    "        \n",
    "        print(\"*\" * 50)\n",
    "        print(run_name)\n",
    "        print(\"*\" * 50)\n",
    "        \n",
    "        try:\n",
    "            #--------------------------------------------------\n",
    "            # Save the stats to\n",
    "            result_file = f\"{RES_FILE_DIR}/{exp_name}/{run_name}/validation_results_{VAL_BATCHES}_batches_{IW_N_SAMPLES}_samples_BS_{BATCH_SIZE}.p\"\n",
    "            if os.path.isfile(result_file):\n",
    "                print(\"Did this one already, continuing.\")\n",
    "                continue\n",
    "\n",
    "            #--------------------------------------------------\n",
    "            # Get data that this model was trained on\n",
    "            if \"optimus\" in row[\"dataset\"].lower():\n",
    "                dataset_name = \"optimus_yelp\"\n",
    "            elif \"yelp\" in row[\"dataset\"].lower():\n",
    "                dataset_name = \"yelp\"\n",
    "            else:\n",
    "                dataset_name = \"ptb_text_only\"\n",
    "\n",
    "            # Load relevant data\n",
    "            data = NewsData(dataset_name=dataset_name, tokenizer_name=\"roberta\", batch_size=BATCH_SIZE, \n",
    "                            num_workers=3, pin_memory=True, max_seq_len=64, device=DEVICE)\n",
    "            val_loader = data.val_dataloader(shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "            # Determine number of batches to evaluate\n",
    "            if VAL_BATCHES > len(val_loader) or VAL_BATCHES < 0:\n",
    "                MAX_BATCHES = len(val_loader)\n",
    "            else:\n",
    "                MAX_BATCHES = VAL_BATCHES\n",
    "\n",
    "            #--------------------------------------------------\n",
    "            # Get best checkpoint and loss term manager\n",
    "            path, best_epoch = get_best_checkpoint(run_name, exp_name=exp_name)\n",
    "            loss_term_manager = load_from_checkpoint(path, world_master=True, ddp=False, device_name=\"cuda:0\", \n",
    "                                                     evaluation=True, return_loss_term_manager=True)\n",
    "\n",
    "            res = {\n",
    "                \"lens_x_gen\":[], # batch\n",
    "                \"lens\":[],\n",
    "\n",
    "                \"iw_ll_x_gen_p_w\":[], # batch\n",
    "                \"iw_ll_x_gen\": [],\n",
    "                \"iw_ll_p_w\": [],\n",
    "                \"iw_ll\": [],\n",
    "\n",
    "                \"reconstruction_loss\": [], # float\n",
    "                \"exact_match\": [],\n",
    "                \"cross_entropy_per_word\": [],\n",
    "\n",
    "                \"log_q_z_x\":[], # batch x samples\n",
    "                \"log_p_z\":[],\n",
    "                \"log_p_x_z\":[],\n",
    "\n",
    "                \"latent_z\": [], # batch x latent_dim\n",
    "                \"mu\":[],\n",
    "                \"logvar\": [],\n",
    "\n",
    "                \"kl_analytical\": [], # float\n",
    "                \"fb_kl_analytical\": [],\n",
    "                \"elbo\": [],\n",
    "                \"marginal KL\": [],\n",
    "                \"dim_KL\": [], \n",
    "                \"TC\": [],\n",
    "                \"MI\": [],\n",
    "                \"mmd_loss\":[],\n",
    "\n",
    "                \"std_x_std\": [], # float\n",
    "                \"mean_x_std\": [],\n",
    "                \"std_x_mu\": [],\n",
    "                \"mean_x_mu\": [],\n",
    "                \"std_z_std\": [],\n",
    "                \"std_z_mu\":[],\n",
    "            }\n",
    "\n",
    "\n",
    "            #--------------------------------------------------\n",
    "            # Make predictions\n",
    "            for batch_i, batch in enumerate(val_loader):\n",
    "                print(f\"Batch {batch_i+1:3d}/{MAX_BATCHES:3d}\", end=\"\\r\")\n",
    "\n",
    "                batch = transfer_batch_to_device(batch)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    out = loss_term_manager(batch[\"input_ids\"], batch[\"attention_mask\"], \n",
    "                                            return_exact_match=True, decoder_only=False, eval_iw_ll_x_gen=True,\n",
    "                                            return_posterior_stats=True, device_name=DEVICE, iw_ll_n_samples=IW_N_SAMPLES,\n",
    "                                            return_attention_to_latent=False, train=False, max_seq_len_x_gen=64)\n",
    "\n",
    "                for k, v in out.items():\n",
    "                    if k in res:\n",
    "                        if torch.is_tensor(v):\n",
    "                            res[k].append(v.cpu())\n",
    "                        else:\n",
    "                            res[k].append(v)\n",
    "\n",
    "                if batch_i + 1 == MAX_BATCHES:\n",
    "                    break\n",
    "\n",
    "            #--------------------------------------------------\n",
    "            # Gather predictions\n",
    "            for k, v in res.items():\n",
    "                if torch.is_tensor(v[0]):\n",
    "                    if v[0].dim() > 0:\n",
    "                        res[k] = torch.cat(v).cpu()\n",
    "                    else:\n",
    "                        res[k] = torch.stack(v).cpu()\n",
    "\n",
    "            #--------------------------------------------------\n",
    "            # Add log q z on whole validation set (only one latent sample z per data point x is needed)\n",
    "            res[\"log_q_z\"], _ = approximate_log_q_z(res[\"mu\"], res[\"logvar\"], res[\"latent_z\"][:, 0, :], method=\"all\", prod_marginals=False, reduce_mean=False)\n",
    "            res[\"log_q_z_mean\"] = res[\"log_q_z\"].mean()\n",
    "\n",
    "            #--------------------------------------------------\n",
    "            # Dump the result in a pickle\n",
    "            pickle.dump(res, open(result_file, \"wb\"))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"** ERROR for :\", run_name, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "thesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
