{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is file!\n",
      "train 42068\n",
      "validation 3370\n",
      "test 3761\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append(\"../\")\n",
    "\n",
    "# Functions from my code\n",
    "from dataset_wrappper import NewsData\n",
    "from utils_train import transfer_batch_to_device\n",
    "from utils_evaluation import tokenizer_batch_decode\n",
    "from run_validation import load_model_for_eval\n",
    "\n",
    "# General libs\n",
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizerFast\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.reload_library()\n",
    "plt.style.use('thesis_style')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Globals\n",
    "BATCH_SIZE = 2\n",
    "DEVICE = \"cuda:0\"\n",
    "CHECKPOINT_TYPE = \"best\" # else \"best\"\n",
    "RESULT_DIR = Path(\"result-files\")\n",
    "\n",
    "# Data\n",
    "data = NewsData(batch_size=BATCH_SIZE, tokenizer_name=\"roberta\", dataset_name=\"ptb_text_only\", max_seq_len=64)\n",
    "# data = NewsData(batch_size=BATCH_SIZE, tokenizer_name=\"roberta\", dataset_name=\"cnn_dailymail\", max_seq_len=64)\n",
    "validation_loader = data.val_dataloader(batch_size=BATCH_SIZE)\n",
    "train_loader = data.train_dataloader()\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t NZ-32 | FB-0.00\n",
      "1 \t NZ-32 | FB-0.25\n",
      "2 \t NZ-32 | FB-0.50\n",
      "3 \t NZ-32 | FB-0.75\n",
      "4 \t NZ-32 | FB-1.00\n",
      "5 \t NZ-32 | FB-1.50\n",
      "6 \t NZ-32 | autoencoder\n",
      "7 \t NZ-64 | FB-0.00\n",
      "8 \t NZ-64 | FB-0.25\n",
      "9 \t NZ-64 | FB-0.50\n",
      "10 \t NZ-64 | FB-0.75\n",
      "11 \t NZ-64 | FB-1.00\n",
      "12 \t NZ-64 | FB-1.50\n",
      "13 \t NZ-64 | autoencoder\n",
      "14 \t NZ-68 | autoencoder\n",
      "15 \t NZ-68 | autoencoder\n",
      "16 \t NZ-68 | autoencoder\n",
      "17 \t NZ-68 | autoencoder\n",
      "18 \t NZ-68 | autoencoder\n",
      "19 \t NZ-68 | autoencoder\n",
      "20 \t NZ-68 | autoencoder\n"
     ]
    }
   ],
   "source": [
    "def get_clean_name(run_name):\n",
    "    latent_size = run_name.split(\"-\")[4][-2:]\n",
    "    if \"autoencoder\" in run_name:\n",
    "        FB = \"autoencoder\"\n",
    "    else:\n",
    "        FB = run_name.split(\"-\")[6]\n",
    "        if len(FB) == 3:\n",
    "            FB += \"0\"\n",
    "        FB = \"FB-\" + FB\n",
    "    clean_name = f\"NZ-{latent_size} | {FB}\"\n",
    "    return clean_name\n",
    "\n",
    "PTB_run_name_paths = {}\n",
    "for r in os.listdir(\"../Runs\"):\n",
    "    if \"PTB\" in r:\n",
    "        path = Path(\"../Runs\") / r / f\"checkpoint-{CHECKPOINT_TYPE}.pth\"\n",
    "        PTB_run_name_paths[r] = path\n",
    "\n",
    "clean_names = []\n",
    "for r in PTB_run_name_paths.keys():\n",
    "#     print(get_clean_name(r))\n",
    "    clean_names.append(get_clean_name(r))\n",
    "    \n",
    "clean_names = sorted(clean_names)\n",
    "for i, n in enumerate(clean_names):\n",
    "    print(i, \"\\t\", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Collect latents of the whole train set for a few runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load latents from file train_latents.pickle\n"
     ]
    }
   ],
   "source": [
    "runs =  [\n",
    " \"2021-02-03-PTB-latent32-FB-0.5-run-09:31:02\", \n",
    " \"2021-02-03-PTB-latent32-FB-1.50-run-12:13:36\",\n",
    " \"2021-02-03-PTB-latent32-autoencoder-run-17:30:41\"]\n",
    "\n",
    "result_file = \"train_latents.pickle\"\n",
    "\n",
    "if os.path.isfile(result_file):\n",
    "    print(f\"Load latents from file {result_file}\")\n",
    "    latents_runs = pickle.load( open( result_file, \"rb\" ) )\n",
    "else: \n",
    "    print(f\"Compute latents on train set...\")\n",
    "    latents_runs = {}\n",
    "\n",
    "    for r, p in PTB_run_name_paths.items():\n",
    "\n",
    "        if r not in runs:\n",
    "            continue\n",
    "\n",
    "        print(get_clean_name(r))\n",
    "\n",
    "        latent_size = int(r.split(\"-\")[4][-2:])\n",
    "        vae_model = load_model_for_eval(p, device_name=DEVICE, \n",
    "                                        latent_size=latent_size, \n",
    "                                        add_latent_via_memory=True,\n",
    "                                        add_latent_via_embeddings=False, \n",
    "                                        do_tie_weights=True, \n",
    "                                        do_tie_embedding_spaces=True,\n",
    "                                        add_decoder_output_embedding_bias=False)\n",
    "\n",
    "        latents, mus, logvars, strings = [], [], [], []\n",
    "\n",
    "        for batch_i, batch in enumerate(train_loader):\n",
    "\n",
    "            print(f\"Batch {batch_i:3d} /{len(train_loader):3d}\", end=\"\\r\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch = transfer_batch_to_device(batch, device_name=DEVICE)\n",
    "\n",
    "                enc_out = vae_model.encoder.encode(batch[\"input_ids\"], batch[\"attention_mask\"], \n",
    "                                                   n_samples=1,\n",
    "                                                   hinge_kl_loss_lambda=0.0,\n",
    "                                                   return_log_q_z_x=False,\n",
    "                                                   return_log_p_z=False,\n",
    "                                                   return_embeddings=False)\n",
    "\n",
    "                latents.append(enc_out[\"latent_z\"].cpu())\n",
    "                mus.append(enc_out[\"mu\"].cpu())\n",
    "                logvars.append(enc_out[\"logvar\"].cpu())\n",
    "                strings.extend(tokenizer_batch_decode(batch[\"input_ids\"].cpu(), tokenizer))\n",
    "\n",
    "        latents = torch.cat(latents, dim=0)\n",
    "        mus = torch.cat(mus, dim=0)\n",
    "        logvars = torch.cat(logvars, dim=0)\n",
    "\n",
    "        latents_runs[r] = {\n",
    "            \"latents\": latents,\n",
    "            \"mus\": mus,\n",
    "            \"logvars\": logvars,\n",
    "            \"strings\": strings\n",
    "        }\n",
    "\n",
    "    pickle.dump( latents_runs, open( result_file, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Replacing linear output layer with one without bias!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta_new.VaeDecoderRobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_from_checkpoint() got an unexpected keyword argument 'do_tie_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-df67583fc5a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m _, _, vae_model, _, _, _, _ = utils_train.load_from_checkpoint(vae_model, p, \n\u001b[1;32m     34\u001b[0m                                                                \u001b[0mdo_tie_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_tie_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                                                do_tie_weights=do_tie_weights)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mvae_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_from_checkpoint() got an unexpected keyword argument 'do_tie_embeddings'"
     ]
    }
   ],
   "source": [
    "# Load the correct model\n",
    "# r = \"2021-02-03-PTB-latent32-FB-1.50-run-12:13:36\"\n",
    "\n",
    "r = \"2021-02-03-PTB-latent32-autoencoder-run-17:30:41\"\n",
    "latent_size = int(r.split(\"-\")[4][-2:])\n",
    "p = PTB_run_name_paths[r]\n",
    "\n",
    "import train\n",
    "import utils_train\n",
    "\n",
    "# latent_size = 768\n",
    "\n",
    "# p = \"/home/cbarkhof/code-thesis/NewsVAE/Runs/23NOV-AUTOENCODER-MemoryOnly-run-2020-11-23-19:10:58/checkpoint-50000.pth\"\n",
    "# p = \"/home/cbarkhof/code-thesis/NewsVAE/Runs/13JAN-exp6-AUTO-ENCODER-Higher-Linear-Sched-run-2021-01-13-14:21:29/checkpoint-54000.pth\"\n",
    "    \n",
    "    \n",
    "do_tie_embeddings = True\n",
    "do_tie_weights = True\n",
    "add_latent_via_memory = True\n",
    "add_latent_via_embeddings = False\n",
    "latent_size = 32\n",
    "add_decoder_output_embedding_bias = False\n",
    "vae_model = train.get_model_on_device(device_name=DEVICE,\n",
    "                                      latent_size=latent_size,\n",
    "                                      gradient_checkpointing=False,\n",
    "                                      add_latent_via_memory=add_latent_via_memory,\n",
    "                                      add_latent_via_embeddings=add_latent_via_embeddings,\n",
    "                                      do_tie_weights=do_tie_weights,\n",
    "                                      do_tie_embedding_spaces=do_tie_embeddings,\n",
    "                                      world_master=True,\n",
    "                                      add_decoder_output_embedding_bias=add_decoder_output_embedding_bias)\n",
    "\n",
    "_, _, vae_model, _, _, _, _ = utils_train.load_from_checkpoint(vae_model, p, \n",
    "                                                               do_tie_embeddings=do_tie_embeddings, \n",
    "                                                               do_tie_weights=do_tie_weights)\n",
    "\n",
    "vae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbarkhof/.conda/envs/thesisenv/lib/python3.6/site-packages/datasets/arrow_dataset.py:847: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/cbarkhof/.conda/envs/thesisenv/lib/python3.6/site-packages/datasets/arrow_dataset.py:847: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/cbarkhof/.conda/envs/thesisenv/lib/python3.6/site-packages/datasets/arrow_dataset.py:847: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/cbarkhof/.conda/envs/thesisenv/lib/python3.6/site-packages/datasets/arrow_dataset.py:847: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([2, 36])\n",
      "dict_keys(['mu', 'logvar', 'latent_z', 'kl_loss', 'hinge_kl_loss', 'mmd_loss', 'log_q_z_x', 'log_p_z', 'word_embeddings'])\n",
      "torch.Size([2, 36, 768])\n",
      "dict_keys(['cross_entropy', 'cross_entropy_per_word', 'predictions', 'exact_match', 'attention_probs', 'attention_to_latent', 'hidden_states', 'probabilities', 'last_hidden_state', 'logits', 'past_key_values', 'cross_attentions'])\n",
      "torch.Size([2, 36, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in validation_loader:\n",
    "        batch = transfer_batch_to_device(batch, device_name=DEVICE)\n",
    "        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "        print(\"input_ids\", input_ids.shape)\n",
    "        #out = vae_model(input_ids=input_ids, attention_mask=attention_mask, beta=1.0, return_cross_entropy=False)\n",
    "        enc_out = vae_model.encoder.encode(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], return_embeddings=True)\n",
    "        print(enc_out.keys())\n",
    "        print(enc_out[\"word_embeddings\"].shape)\n",
    "        \n",
    "        dec_out = vae_model.decoder(enc_out[\"latent_z\"], input_ids, attention_mask,\n",
    "                                    return_cross_entropy=False,\n",
    "                                    return_last_hidden_state=True)\n",
    "        \n",
    "        print(dec_out.keys())\n",
    "        \n",
    "        print(dec_out[\"last_hidden_state\"].shape)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vae_model.eval()\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     for batch in validation_loader:\n",
    "#         batch = transfer_batch_to_device(batch, device_name=DEVICE)\n",
    "#         input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "\n",
    "#         # Encode\n",
    "#         enc_out = vae_model.encoder.encode(input_ids, attention_mask, \n",
    "#                                            n_samples=1, hinge_kl_loss_lambda=0.0,\n",
    "#                                            return_log_q_z_x=False, return_log_p_z=False, \n",
    "#                                            return_embeddings=False)\n",
    "#         latent_to_decoder_output = vae_model.decoder.latent_to_decoder(enc_out[\"latent_z\"])\n",
    "        \n",
    "#         # Decode and return cache\n",
    "#         dec_out = vae_model.decoder.model(latent_to_decoder_output=latent_to_decoder_output,\n",
    "#                                           input_ids=input_ids[:, :5],\n",
    "#                                           attention_mask=None,\n",
    "#                                           return_cross_entropy=False,\n",
    "#                                           return_predictions=True,\n",
    "#                                           use_cache=True)\n",
    "        \n",
    "#         preds_forward_1 = dec_out[\"predictions\"]\n",
    "        \n",
    "#         past_key_values = dec_out[\"past_key_values\"]\n",
    "# #         print(past_key_values[0][0].shape)\n",
    "#         # cut off the latent\n",
    "        \n",
    "#         past_key_values = tuple([tuple([pair[0][:, :, 1:3, :], pair[1][:, :, 1:3, :]]) for pair in past_key_values])\n",
    "#         print(\"**\", past_key_values[0][0].shape)\n",
    "        \n",
    "#         # Now check whether the predictions for second foward with cache\n",
    "#         dec_out = vae_model.decoder.model(latent_to_decoder_output=latent_to_decoder_output,\n",
    "#                                           input_ids=input_ids[:, 2:10],\n",
    "#                                           attention_mask=None,\n",
    "#                                           return_cross_entropy=False,\n",
    "#                                           return_predictions=True,\n",
    "#                                           past_key_values=past_key_values,\n",
    "#                                           use_cache=True)\n",
    "        \n",
    "        \n",
    "#         print(preds_forward_1)\n",
    "#         print(dec_out[\"predictions\"])\n",
    "        \n",
    "        \n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pytorch_lightning import seed_everything\n",
    "\n",
    "# seed_everything(0)\n",
    "\n",
    "# def naive_auto_regressive_reconstruct(vae_model, input_ids, attention_mask, tokenizer):\n",
    "#     vae_model.eval()\n",
    "    \n",
    "#     batch_size = input_ids.shape[0]\n",
    "#     generated_so_far = torch.tensor([[tokenizer.bos_token_id, tokenizer.eos_token_id] for _ in range(batch_size)]).to(DEVICE)\n",
    "    \n",
    "#     print(\"Original:\\n\")\n",
    "#     for t in tokenizer_batch_decode(input_ids.cpu(), tokenizer):\n",
    "#         print(t)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "        \n",
    "            \n",
    "#         enc_out = vae_model.encoder.encode(input_ids, attention_mask)\n",
    "        \n",
    "# #         print(\"\\n\\nAuto-regressive forward VAE:\\n\")\n",
    "# #         for t in tokenizer_batch_decode(out[\"predictions\"].cpu(), tokenizer):\n",
    "# #             print(t)\n",
    "            \n",
    "#         latent_to_decoder_output = vae_model.decoder.latent_to_decoder(enc_out[\"latent_z\"])\n",
    "        \n",
    "#         test_caches = []\n",
    "        \n",
    "#         # do a naive recurrent forward pass\n",
    "#         for i in range(10):\n",
    "#             dec_out = vae_model.decoder(input_ids=generated_so_far, attention_mask=None, \n",
    "#                                         latent_z=enc_out[\"latent_z\"], return_predictions=True,\n",
    "#                                         return_cross_entropy=False)\n",
    "            \n",
    "#              # Forward the decoder\n",
    "#             dec_out = vae_model.decoder.model(latent_to_decoder_output=latent_to_decoder_output,\n",
    "#                                               input_ids=generated_so_far,\n",
    "#                                               attention_mask=None,\n",
    "#                                               return_cross_entropy=False,\n",
    "#                                               return_predictions=True,\n",
    "#                                               use_cache=False)\n",
    "            \n",
    "            \n",
    "#             new_preds = dec_out['predictions'][:, -1]\n",
    "            \n",
    "#             # Concat into <last prediction> </s> format for next round\n",
    "#             generated_so_far = torch.cat(\n",
    "#                 (generated_so_far[:, :-1], new_preds.unsqueeze(1), generated_so_far[:, -1].unsqueeze(1)), dim=1)   \n",
    "            \n",
    "#         latent_z = enc_out[\"latent_z\"]\n",
    "        \n",
    "#         dec_out_2 = vae_model.decoder.autoregressive_decode(latent_z, max_seq_len=10,\n",
    "#                                                             return_predictions=True,\n",
    "#                                                             return_probabilities=False,\n",
    "#                                                             return_logits=False,\n",
    "#                                                             nucleus_sampling=False,\n",
    "#                                                             device_name=\"cuda:0\")\n",
    "        \n",
    "#         out_3 = vae_model(input_ids, 0.0, attention_mask,\n",
    "#                           auto_regressive=True,\n",
    "#                           return_predictions=True)\n",
    "        \n",
    "#         print(generated_so_far[:, 1:-1].shape, dec_out_2[\"predictions\"].shape, out_3[\"predictions\"].shape)\n",
    "        \n",
    "#         print(out_3['predictions'][:, :10])\n",
    "#         print(generated_so_far[:, 1:-1])\n",
    "#         print(dec_out_2[\"predictions\"])\n",
    "        \n",
    "#         print(tokenizer_batch_decode(generated_so_far[:, 1:-1], tokenizer))\n",
    "#         print(tokenizer_batch_decode(dec_out_2[\"predictions\"], tokenizer))\n",
    "        \n",
    "        \n",
    "# #         print(\"\\n\\nNaive autoregressive forward:\\n\")\n",
    "# #         for t in tokenizer_batch_decode(generated_so_far.cpu(), tokenizer):\n",
    "# #             print(t)\n",
    "# vae_model.eval()\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     for batch in validation_loader:\n",
    "#         batch = transfer_batch_to_device(batch, device_name=DEVICE)\n",
    "#         input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "#         naive_auto_regressive_reconstruct(vae_model, input_ids, attention_mask, tokenizer)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbarkhof/.conda/envs/thesisenv/lib/python3.6/site-packages/datasets/arrow_dataset.py:847: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/cbarkhof/.conda/envs/thesisenv/lib/python3.6/site-packages/datasets/arrow_dataset.py:847: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/cbarkhof/.conda/envs/thesisenv/lib/python3.6/site-packages/datasets/arrow_dataset.py:847: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/cbarkhof/.conda/envs/thesisenv/lib/python3.6/site-packages/datasets/arrow_dataset.py:847: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "-   consumers may want to move their telephones a little closer to the tv set\n",
      "-   <unk> <unk> watching abc's monday night football can now vote during <unk> for the greatest play in N years from among four or five \n",
      "\n",
      "Reconstructed\n",
      "-   Consumers may want to move their televisions a little closer to the telephones set to their tv box. Telephones may want to move a little closer to the tv set\n",
      "-   Mathawan  saying can remember overeating during this second greatest football season for  four years in the night from 'Nsca can scout or  downloading omens\n",
      "-----\n",
      "Original\n",
      "-   two weeks ago viewers of several nbc <unk> consumer segments started calling a N number for advice on various <unk> issues\n",
      "-   and the new syndicated reality show hard copy records viewers'opinions for possible airing on the next day's show\n",
      "\n",
      "Reconstructed\n",
      "-   Two n viewers of several weeks  number one announcements started discussing a Nbc  dietary advice for those on various levels of internet\n",
      "-   And the new syndicated show copies hard copies 'the current reality show possible for views on'saying the day' '\n",
      "-----\n",
      "Original\n",
      "-   interactive telephone technology has taken a new leap in <unk> and television programmers are racing to exploit the possibilities\n",
      "-   eventually viewers may grow <unk> with the technology and <unk> the cost\n",
      "\n",
      "Reconstructed\n",
      "-   Interactive telephone technology has taken a new leap in  television racing and scientists are scrambling to exploit the possibilities the\n",
      "-   eventually viewers may grow  technologically with the mascot and  the technology  and may make the cost of \n",
      "-----\n",
      "Original\n",
      "-   but right now programmers are figuring that viewers who are busy dialing up a range of services may put down their <unk> control <unk> and stay <unk>\n",
      "-   we've been spending a lot of time in los angeles talking to tv production people says mike parks president of call interactive which supplied technology for both\n",
      "\n",
      "Reconstructed\n",
      "-   But dialers who are figuring out that programmers are right temporarily put down some of their services  may control a busy range  messaging and download-only websites. While viewers who are able to dial up the internet\n",
      "-   We've spent a lot of time 'we tv in genial otters speaking to call animals creator of marketing technology' where LA park chief Andy Colc reports both for sports nab and technology'soci\n",
      "-----\n",
      "Original\n",
      "-   with the competitiveness of the television market these days everyone is looking for a way to get viewers more excited\n",
      "-   one of the leaders behind the expanded use of N numbers is call interactive a joint venture of giants american express co. and american telephone & telegraph\n",
      "\n",
      "Reconstructed\n",
      "-   With the competitiveness of the television market these days everyone is looking for a way to get more viewers excited about the season. With parents today being one of the way many\n",
      "-   One of the leaders behind expanded numbers the N.O. is call of interactive interplanetary express company and a titan of US telephone co-workers Warren Buffett\n",
      "-----\n",
      "Original\n",
      "-   formed in august the venture <unk> at&t's newly expanded N service with N <unk> computers in american express's omaha neb\n",
      "-   other long-distance carriers have also begun marketing enhanced N service and special consultants are <unk> up to exploit the new tool\n",
      "\n",
      "Reconstructed\n",
      "-   Microsoft augmentation at the venture  in NUSTIN with 'slightly expanded express services N.cola in america express service'saha nebunda omens\n",
      "-   Both middle-distance specialists have  begun enhanced Naming service and new special carrier is also up to explore the long-term benefits the company has utilised to recruit overseas workers\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed_everything(0)\n",
    "\n",
    "def naive_auto_regressive_reconstruct(vae_model, input_ids, attention_mask, tokenizer):\n",
    "    \n",
    "#     batch_size = input_ids.shape[0]\n",
    "#     generated_so_far = torch.tensor([[tokenizer.bos_token_id, tokenizer.eos_token_id] for _ in range(batch_size)]).to(DEVICE)\n",
    "    \n",
    "#     enc_out = vae_model.encoder.encode(input_ids, attention_mask)\n",
    "#     latent_to_decoder_output = vae_model.decoder.latent_to_decoder(enc_out[\"latent_z\"])\n",
    "    \n",
    "#     # Do a naive recurrent forward pass\n",
    "#     for i in range(32):\n",
    "\n",
    "#          # Forward the decoder\n",
    "#         dec_out = vae_model.decoder.model(latent_to_decoder_output=latent_to_decoder_output,\n",
    "#                                           input_ids=generated_so_far,\n",
    "#                                           attention_mask=None,\n",
    "#                                           return_cross_entropy=False,\n",
    "#                                           return_predictions=True,\n",
    "#                                           use_cache=False)\n",
    "\n",
    "\n",
    "#         new_preds = dec_out['predictions'][:, -1]\n",
    "\n",
    "#         # Concat into <last prediction> </s> format for next round\n",
    "#         generated_so_far = torch.cat(\n",
    "#             (generated_so_far[:, :-1], new_preds.unsqueeze(1), generated_so_far[:, -1].unsqueeze(1)), dim=1)   \n",
    "        \n",
    "    \n",
    "    out = vae_model(input_ids, 0.0, attention_mask, auto_regressive=True, return_predictions=True)\n",
    "#     out_2 = vae_model.decoder.autoregressive_decode(enc_out[\"latent_z\"], max_seq_len=32,\n",
    "#                                                     return_predictions=True,\n",
    "#                                                     return_probabilities=False,\n",
    "#                                                     return_logits=False)\n",
    "    \n",
    "    t0 = tokenizer_batch_decode(input_ids[:, :32].cpu(), tokenizer)\n",
    "#     t1 = tokenizer_batch_decode(generated_so_far[:, :], tokenizer)\n",
    "    t2 = tokenizer_batch_decode(out[\"predictions\"][:, :], tokenizer)\n",
    "#     t3 = tokenizer_batch_decode(out_2[\"predictions\"][:, :], tokenizer)\n",
    "    \n",
    "    for i, ts in enumerate([t0, t2]):\n",
    "        if i == 0:\n",
    "            print(\"Original\")\n",
    "        else:\n",
    "            print(\"\\nReconstructed\")\n",
    "        \n",
    "        for t in ts:\n",
    "            if \"<s>\" in t:\n",
    "                t = t[3:]\n",
    "            index_end = t.find(\"</s>\")\n",
    "            if index_end != -1:\n",
    "                t = t[:index_end]\n",
    "            print(\"-  \", t)\n",
    "    print(\"-----\")\n",
    "        \n",
    "\n",
    "vae_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for batch_i, batch in enumerate(validation_loader):\n",
    "        batch = transfer_batch_to_device(batch, device_name=DEVICE)\n",
    "        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "        naive_auto_regressive_reconstruct(vae_model, input_ids, attention_mask, tokenizer)\n",
    "    \n",
    "        if batch_i == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Compute neares neighbors of samples from the priors to latents from the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some samples from the prior\n",
    "\n",
    "latent_size = 32\n",
    "scale = 1.0  # <-- standard normal\n",
    "n_samples = 10\n",
    "loc = torch.zeros(latent_size, device=\"cpu\")\n",
    "std = torch.ones(latent_size, device=\"cpu\") * scale\n",
    "prior_dist = torch.distributions.normal.Normal(loc, std)\n",
    "prior_samples = prior_dist.sample((n_samples,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "NZ-32 | FB-0.50\n",
      "**************************************************\n",
      "\n",
      "Loading model...\n",
      "OLD FILE decoder_roberta.py activated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Decoder_RobertaForCausalLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Decoder_RobertaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Decoder_RobertaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VAE_Decoder_RobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing linear output layer with one without bias!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing VAE_Encoder_RobertaModel: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VAE_Encoder_RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VAE_Encoder_RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tying encoder decoder RoBERTa checkpoint weights!\n",
      "<class 'modules.decoder_roberta.VAE_Decoder_RobertaModel'> and <class 'modules.encoder_roberta.VAE_Encoder_RobertaModel'> are not equal. In this case make sure that all encoder weights are correctly initialized. \n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "Tying embedding spaces!\n",
      "Done model...\n",
      "Loading VAE_model, optimizer and scheduler from ../Runs/2021-02-03-PTB-latent32-FB-0.5-run-09:31:02/checkpoint-best.pth\n",
      "Removing module string from state dict from checkpoint\n",
      "Checkpoint global_step: best, epoch: 37, best_valid_loss: 84.10610651086878\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'use_cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e100ed335cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m                                                           \u001b[0mreduce_batch_dim_exact_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"none\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                                                           \u001b[0mreduce_batch_dim_ce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                                                           device_name=DEVICE)\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# The first prediction is the one belonging to the sample from prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code-thesis/NewsVAE/modules/decoder.py\u001b[0m in \u001b[0;36mautoregressive_decode\u001b[0;34m(self, latent_z, max_seq_len, labels, return_exact_match, return_cross_entropy, reduce_seq_dim_ce, reduce_batch_dim_ce, reduce_seq_dim_exact_match, reduce_batch_dim_exact_match, return_output_word_embeddings, return_attention_probs, return_attention_to_latent, return_hidden_states, return_last_hidden_state, tokenizer, return_predictions, return_probabilities, return_logits, nucleus_sampling, top_k, top_p, device_name)\u001b[0m\n\u001b[1;32m    250\u001b[0m                                       \u001b[0mnucleus_sampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnucleus_sampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                                       \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                                       top_p=top_p)\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# print(\"decoder_outs[predictions].shape\", decoder_outs[\"predictions\"].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'use_cache'"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "n_neighbors = 5\n",
    "\n",
    "runs =  [\n",
    " \"2021-02-03-PTB-latent32-FB-0.5-run-09:31:02\", \n",
    " \"2021-02-03-PTB-latent32-FB-1.50-run-12:13:36\",\n",
    " \"2021-02-03-PTB-latent32-autoencoder-run-17:30:41\"]\n",
    "\n",
    "for r in runs:\n",
    "    print(\"*\"*50)\n",
    "    print(get_clean_name(r))\n",
    "    print(\"*\"*50)\n",
    "    print()\n",
    "    \n",
    "    # Get the mean of the posteriors as the encodings\n",
    "    posterior_means = latents_runs[r][\"mus\"]\n",
    "    \n",
    "    # Load the correct model\n",
    "    latent_size = int(r.split(\"-\")[4][-2:])\n",
    "    p = PTB_run_name_paths[r]\n",
    "    vae_model = load_model_for_eval(p, device_name=DEVICE, \n",
    "                                    latent_size=latent_size, \n",
    "                                    add_latent_via_memory=True,\n",
    "                                    add_latent_via_embeddings=False, \n",
    "                                    do_tie_weights=True, \n",
    "                                    do_tie_embedding_spaces=True,\n",
    "                                    add_decoder_output_embedding_bias=False)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"-\"*30)\n",
    "\n",
    "    # Make a brute-forced nearest neighbor graph of the validation set, find 10 neighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='brute').fit(posterior_means.numpy())\n",
    "\n",
    "    # Get the closest data train posterior means to a sample from the prior\n",
    "    # distances between those and indices of the initial graph (corresponding to latents matrix)\n",
    "    distances, indices = nbrs.kneighbors(prior_samples.cpu().numpy())\n",
    "\n",
    "    # Go over the samples in the priors and evaluate the closest train posteriors\n",
    "    for s_i in range(len(prior_samples)):\n",
    "\n",
    "        sample_from_prior = prior_samples[s_i, :]\n",
    "\n",
    "        closest_post_means_train = []\n",
    "        closest_string_train = []\n",
    "\n",
    "        ds = []\n",
    "        for idx, d in zip(indices[s_i], distances[s_i]):\n",
    "\n",
    "            closest_post_means_train.append(posterior_means[idx, :])\n",
    "            closest_string_train.append(latents_runs[r][\"strings\"][idx])\n",
    "            ds.append(d)\n",
    "\n",
    "        sample_and_train_neighbors = torch.stack([sample_from_prior] + closest_post_means_train, dim=0).to(DEVICE)\n",
    "        \n",
    "        vae_model.eval()\n",
    "        with torch.no_grad():\n",
    "            dec = vae_model.decoder.autoregressive_decode(sample_and_train_neighbors,\n",
    "                                                          labels=None,\n",
    "                                                          max_seq_len=32,\n",
    "                                                          return_exact_match=False,\n",
    "                                                          return_cross_entropy=False,\n",
    "                                                          return_attention_probs=False,\n",
    "                                                          return_attention_to_latent=False,\n",
    "                                                          return_hidden_states=False,\n",
    "                                                          return_last_hidden_state=False,\n",
    "                                                          return_predictions=True,\n",
    "                                                          return_probabilities=False,\n",
    "                                                          return_output_word_embeddings=False,\n",
    "                                                          return_logits=False,\n",
    "                                                          tokenizer=tokenizer,\n",
    "                                                          nucleus_sampling=False, # <-- to check variability from the latent turn this off\n",
    "                                                          reduce_seq_dim_ce=\"sum\",\n",
    "                                                          reduce_seq_dim_exact_match=\"none\",\n",
    "                                                          reduce_batch_dim_exact_match=\"none\",\n",
    "                                                          reduce_batch_dim_ce=\"mean\",\n",
    "                                                          device_name=DEVICE)\n",
    "        \n",
    "        # The first prediction is the one belonging to the sample from prior\n",
    "        # the rest are reconstructions of the posterior mean encodings\n",
    "        \n",
    "        # Post process the strings, cutting of padding etc.\n",
    "        text = tokenizer_batch_decode(dec[\"predictions\"], tokenizer)\n",
    "        N = len(text)\n",
    "        text += closest_string_train\n",
    "        ts = []\n",
    "        for t in text:\n",
    "            if \"<s>\" in t:\n",
    "                t = t[3:]\n",
    "            index_end = t.find(\"</s>\")\n",
    "            if index_end == -1:\n",
    "                ts.append(t)\n",
    "            else:\n",
    "                ts.append(t[:index_end])\n",
    "                \n",
    "        closest_string_train = ts[N:]\n",
    "        text = ts[:N]\n",
    "\n",
    "        print(\"Sample from prior decoded:\\n\", ts[0])\n",
    "        print(\"-\"*60)\n",
    "        print(\"Means from train sample latents that lay closest:\")\n",
    "        for i, (recon, orig) in enumerate(zip(ts[1:], closest_string_train)):\n",
    "            print(f\"{i} -- D = {ds[i]:.2f}\")\n",
    "            print(\"** Auto-regressive reconstruction:\\n\", recon)\n",
    "            print(\"** Original string:\\n\", orig)\n",
    "            print()\n",
    "        print(\"-\"*60)\n",
    "        print(\"-\"*60)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, neighbor_triplets in set_of_neighbor_triplets.items():\n",
    "    print(get_clean_name(r))\n",
    "    \n",
    "    samples = torch.cat(neighbor_triplets, dim=0)\n",
    "    print(samples.shape)\n",
    "    \n",
    "    p = PTB_run_name_paths[r]\n",
    "    vae_model = load_model_for_eval(p, device_name=\"cuda:0\", \n",
    "                                    latent_size=latent_size, \n",
    "                                    add_latent_via_memory=True,\n",
    "                                    add_latent_via_embeddings=False, \n",
    "                                    do_tie_weights=True, \n",
    "                                    do_tie_embedding_spaces=True,\n",
    "                                    add_decoder_output_embedding_bias=False)\n",
    "    \n",
    "    dec = vae_model.decoder.autoregressive_decode(samples,\n",
    "                                                    labels=None,\n",
    "                                                    max_seq_len=32,\n",
    "                                                    return_exact_match=False,\n",
    "                                                    return_cross_entropy=False,\n",
    "                                                    return_attention_probs=False,\n",
    "                                                    return_attention_to_latent=False,\n",
    "                                                    return_hidden_states=False,\n",
    "                                                    return_last_hidden_state=False,\n",
    "                                                    return_predictions=True,\n",
    "                                                    return_probabilities=False,\n",
    "                                                    return_output_word_embeddings=False,\n",
    "                                                    return_logits=False,\n",
    "                                                    tokenizer=tokenizer,\n",
    "                                                    nucleus_sampling=False, # <-- to check variability from the latent turn this off\n",
    "                                                    reduce_seq_dim_ce=\"sum\",\n",
    "                                                    reduce_seq_dim_exact_match=\"none\",\n",
    "                                                    reduce_batch_dim_exact_match=\"none\",\n",
    "                                                    reduce_batch_dim_ce=\"mean\",\n",
    "                                                    device_name=device_name)\n",
    "        \n",
    "        text = tokenizer_batch_decode(dec[\"predictions\"], tokenizer)\n",
    "        ts = []\n",
    "        for t in text:\n",
    "            index_end = t.find(\"</s>\")\n",
    "            if index_end == -1:\n",
    "                ts.append(t)\n",
    "            else:\n",
    "                ts.append(t[:index_end])\n",
    "        for i, t in enumerate(ts):\n",
    "            if i % 3 == 0:\n",
    "                print(\"-\"*50)\n",
    "                \n",
    "            print(i, t)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "device_name = \"cuda:0\"\n",
    "\n",
    "runs =  [\"2021-02-03-PTB-latent32-FB-0.00-run-14:32:09\",\n",
    " \"2021-02-03-PTB-latent32-FB-0.5-run-09:31:02\", \n",
    " \"2021-02-03-PTB-latent32-FB-1.0-run-11:43:17\", \n",
    " \"2021-02-03-PTB-latent32-FB-1.50-run-12:13:36\",\n",
    " \"2021-02-03-PTB-latent32-autoencoder-run-17:30:41\"]\n",
    "\n",
    "text_preds = {}\n",
    "atts_to_latent = {}\n",
    "\n",
    "for r in runs:\n",
    "    \n",
    "    p = PTB_run_name_paths[r]\n",
    "    print(get_clean_name(r))\n",
    "    \n",
    "    latent_size = int(r.split(\"-\")[4][-2:])\n",
    "    \n",
    "    vae_model = load_model_for_eval(p, device_name=\"cuda:0\", \n",
    "                                    latent_size=latent_size, \n",
    "                                    add_latent_via_memory=True,\n",
    "                                    add_latent_via_embeddings=False, \n",
    "                                    do_tie_weights=True, \n",
    "                                    do_tie_embedding_spaces=True,\n",
    "                                    add_decoder_output_embedding_bias=False)\n",
    "    \n",
    "    vae_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "#         prior_sample = vae_model.sample_from_prior(latent_size, n_samples=n_samples, device_name=device_name)\n",
    "        \n",
    "        loc = torch.zeros(latent_size, device=device_name)\n",
    "        scale = torch.ones(latent_size, device=device_name)*2.0\n",
    "        prior_dist = torch.distributions.normal.Normal(loc, scale)\n",
    "        prior_sample = prior_dist.sample((n_samples,))\n",
    "        \n",
    "        prior_sample = torch.zeros(latent_size, device=device_name)\n",
    "        \n",
    "        prior_decoded_ar = vae_model.decoder.autoregressive_decode(\n",
    "                                                    prior_sample,\n",
    "                                                    labels=None,\n",
    "                                                    max_seq_len=32,\n",
    "                                                    return_exact_match=False,\n",
    "                                                    return_cross_entropy=False,\n",
    "                                                    return_attention_probs=False,\n",
    "                                                    return_attention_to_latent=True,\n",
    "                                                    return_hidden_states=False,\n",
    "                                                    return_last_hidden_state=False,\n",
    "                                                    return_predictions=True,\n",
    "                                                    return_probabilities=False,\n",
    "                                                    return_output_word_embeddings=False,\n",
    "                                                    return_logits=False,\n",
    "                                                    tokenizer=tokenizer,\n",
    "                                                    nucleus_sampling=False, # <-- to check variability from the latent turn this off\n",
    "                                                    reduce_seq_dim_ce=\"sum\",\n",
    "                                                    reduce_seq_dim_exact_match=\"none\",\n",
    "                                                    reduce_batch_dim_exact_match=\"none\",\n",
    "                                                    reduce_batch_dim_ce=\"mean\",\n",
    "                                                    device_name=device_name)\n",
    "        \n",
    "        atts_to_latent[r] = prior_decoded_ar[\"attention_to_latent\"].cpu()\n",
    "\n",
    "        prior_decoded_ar_text = tokenizer_batch_decode(prior_decoded_ar[\"predictions\"], tokenizer)\n",
    "        ts = []\n",
    "        for t in prior_decoded_ar_text:\n",
    "            index_end = t.find(\"</s>\")\n",
    "            if index_end == -1:\n",
    "                ts.append(t)\n",
    "            else:\n",
    "                ts.append(t[:index_end])\n",
    "        text_preds[r] = ts\n",
    "\n",
    "for (r, ts), (_, aw) in zip(text_preds.items(), atts_to_latent.items()):\n",
    "    print(\"*\"*40)\n",
    "    print(get_clean_name(r))\n",
    "    print(\"*\"*40)\n",
    "    # batch, n_heads, n_layers, seq_len_query, seq_len_val\n",
    "    # attention_to_latent = attention_probs[:, :, :, :-1, 0]\n",
    "    aw_mean_head_layers = aw.mean(dim=1).mean(dim=1)\n",
    "#     print(aw_mean_head_layers.shape)\n",
    "#     print(aw.shape)\n",
    "    plt.imshow(aw_mean_head_layers.numpy())\n",
    "    \n",
    "    plt.title(\"Average attention to latent over sequence\", y=1.05, size=12)\n",
    "    plt.colorbar()\n",
    "    plt.ylabel(\"Samples in batch\")\n",
    "    plt.xlabel(\"Sequence dimension\")\n",
    "    plt.show()\n",
    "    for i, t in enumerate(ts):\n",
    "        print(i, t)\n",
    "        \n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "thesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
